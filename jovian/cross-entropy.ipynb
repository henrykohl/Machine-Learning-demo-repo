{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrykohl/Machine-Learning-demo-repo/blob/master/jovian/cross-entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlb2kNxnfHw-"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">Cross-Entropy 在資料分類上，有著非常重要的功能。網路上關於在Pytorch中，如何使用封裝好的函數，獲得Cross Entropy的文章相當多，有的過於簡單，有的又過於複雜，深入淺出難易適中，還能從定義聯繫到實例示範（例如，手動計算Cross Entropy與利用Pytorch所獲得的結果，並比較兩結果）的文章更是不多。本文將從Cross Entropy的定義開始，列出出常看到的定義（舉兩個看起來不同的來說明），接著將用一個實際例子，先依據理論手動計算Cross Entropy，然後使用Pytorch，利用不同的封裝函數，示範如何求得Cross Entropy，再互相比較驗證此兩結果。</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feSBNrbTfHxj"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">定義一：$D(\\hat Y, Y)$與定義二：$D(S, L)$ ，兩者的差異，在於前者是mean，而後者是sum，$N$是輸入資料的筆數(也就是batch size)，$\\hat Y_i$是指第$i$筆資料的預測機率分布向量（i.e., $i$是此筆資料的index），例如第$i$筆資料的預測機率分布$\\hat Y_i$是$[0.7, 0.2, 0.1]$，此向量的維數是3，表示number of classes：$C$=3（e.g., 0表示第一類，1表示第二類，2表示第三類），$Y_i$是第$i$筆資料的實際類別標籤（e.g.,若$Y_i$的實際類別為\"0\"，那就是第一類），$Y_i$的one-hot encoded label被表示成：$[1,0,0]$。  \n",
        "再看定義二，$S$就是sample（有另一含意softmax之後會談到）, $S_i$是第$i$筆資料的預測機率分布向量（如同$\\hat Y_i$），$L$就是label，$L_i$是第$i$筆資料的實際類別標籤（如同$Y_i$）。注意，定義二中的 <font color=\"red\">$y$</font>，$S(y)$是假設只有一筆資料，如果有多筆資料，應該要表示成$S(y_i)$，$S(y_i)$才等於$Y_i$，之後用實例說明會更為清楚，$y_i$是第$i$筆資料的類模型權重向量（向量中的值可能有負值，可能大於零，也可能小於零）。</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEQmqzzBfHxp"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">在分類問題中，通過訓練logistic regression model的過程，對每一個training data，我們會得到此data的類權重分布向量，也就是 <font color=\"red\">$y$</font>，現在舉一實例，假設現在的training data set(dw)，包含五筆的權重分布向量，$N$=5，number of classes是3，$C$=3，現在用pytorcht隨機產生。</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E3VjDNP6fHxv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tz8zUZbtfHyA",
        "outputId": "462961d7-1ae8-4a86-d32e-e6c676e6f48d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.5410, -0.2934, -2.1788],\n",
              "        [ 0.5684, -1.0845, -1.3986],\n",
              "        [ 0.4033,  0.8380, -0.7193],\n",
              "        [-0.4033, -0.5966,  0.1820],\n",
              "        [-0.8567,  1.1006, -1.0712]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "dw = torch.randn(5, 3)\n",
        "dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7YOz4GmWfHyK",
        "outputId": "706e0846-55a9-4474-cc9f-100956240a65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "dw.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKBeB4o4fHyN"
      },
      "source": [
        "dw.shape，就是（$N$, $C$），有5筆權重向量或是Scores/Logits向量，類別數為3，第一筆 <font color=\"red\">$y$</font> 就是dw[0]，第二筆 <font color=\"red\">$y$</font> 就是dw[1]，...。在權重向量中，有正值有負值，可能有大於1，也可能小於1。因此權重向量需要透過softmax處理，轉成機率分布向量。softmax的定義如下：\n",
        "\n",
        "<br>\n",
        "\n",
        "$S(y_i)=\\Large \\frac{e^{y_{ij}}}{\\sum_{j}^{C} e^{y_{ij}}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$y_{ij}$ 是指第$i$筆權重向量中的第$j$個的值。根據softmax的定義，可以算出$S(y_i)$，$0\\leq i< N=5$\n",
        "\n",
        "<br>\n",
        "\n",
        "$S(y_0)=\\Large [\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, \\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, \\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$\n",
        "\n",
        "\n",
        "$S(y_1)=\\Large [\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, \\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, \\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$\n",
        "\n",
        "$S(y_2)=\\Large [\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, \\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, \\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$\n",
        "\n",
        "$S(y_3)=\\Large [\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, \\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, \\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$\n",
        "\n",
        "$S(y_4)=\\Large [\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, \\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, \\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "在Pytorch中，可以用torch.nn.functional.softmax或是torch.nn.Softmax（注意大小寫）兩種方式。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Vmj1oPjPfHyS",
        "outputId": "191d6f23-028b-4baa-8ef3-a805fcfe8d55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8446, 0.1349, 0.0205],\n",
              "        [0.7511, 0.1438, 0.1051],\n",
              "        [0.3484, 0.5382, 0.1134],\n",
              "        [0.2762, 0.2277, 0.4961],\n",
              "        [0.1125, 0.7967, 0.0908]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# torch.nn.functional.softmax\n",
        "fsoft = F.softmax(dw, dim=1)\n",
        "fsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MkEmlD3zfHyW",
        "outputId": "77d7c451-dbb6-4c23-8cf4-d53ac9334640",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8446, 0.1349, 0.0205],\n",
              "        [0.7511, 0.1438, 0.1051],\n",
              "        [0.3484, 0.5382, 0.1134],\n",
              "        [0.2762, 0.2277, 0.4961],\n",
              "        [0.1125, 0.7967, 0.0908]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# torch.nn.Softmax\n",
        "nsoft = nn.Softmax(dim=1)\n",
        "nsoft(dw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqrmlYwHfHya"
      },
      "source": [
        "fsoft與nsoft的結果完全一致。接著是很簡單的一步驟，對$S(y_i)$取log，注意在Pytorch中log是以$e$為基底（log是ln）：\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_0))$=$\\Large [{\\small log}\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, {\\small log}\\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, {\\small log}\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_1))$=$\\Large [{\\small log}\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, {\\small log}\\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, {\\small log}\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_2))$=$\\Large [{\\small log}\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, {\\small log}\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, {\\small log}\\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_3))$=$\\Large [{\\small log}\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, {\\small log}\\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, {\\small log}\\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_4))$=$\\Large [{\\small log}\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, {\\small log}\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, {\\small log}\\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UQURbdfzfHyg",
        "outputId": "759a3c1b-a847-4143-fc02-284b3bea2478",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "logfsoft = torch.log(fsoft)\n",
        "logfsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sfIMNl5FfHyj",
        "outputId": "60a42099-dd6f-44a1-984e-3074cd9ecea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "lognsoft = torch.log(nsoft(dw))\n",
        "lognsoft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2p9uWaAfHyl"
      },
      "source": [
        "上述兩個步驟在Pytorch中可以合併成一個步驟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Gs8W9MASfHyn",
        "outputId": "9ea1fb28-ef00-4c03-b145-661641b49f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# torch.nn.functional\n",
        "log_fsoft = F.log_softmax(dw, dim=1)\n",
        "log_fsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SOsiSVWafHyq",
        "outputId": "ef4313d0-0d38-41f6-c4c7-639f33d72562",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# torch.nn\n",
        "log_nsoft = nn.LogSoftmax(dim=1)\n",
        "log_nsoft(dw)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "已知 資料標籤為 **labels**: `[2,2,1,0,1]`，下一個步驟是把 **`labels`** 與 上面取$log$ 的結果 (`log_fsoft` 或 `log_nsoft(dw)`) ，執行 Element-wise multiplication 然後再做 Summation."
      ],
      "metadata": {
        "id": "vwyJNGQiUwDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.tensor([2,2,1,0,1])\n",
        "labels"
      ],
      "metadata": {
        "id": "vb8bbfwLUkmh",
        "outputId": "33d534e8-f523-495a-832a-4031e6d39396",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "假設 $L_0$ 的 label是 **2**, $L_1$ 的 label是 **2**, $L_2$ 的 label是 **1**, $L_3$ 的 label是 **0**, $L_4$ 的 label是 **1**，則他們的one-hot encoded 的結果：\n",
        "\n",
        "<br>\n",
        "\n",
        "$$L_0=[0,0,1]$$ <br>\n",
        "$$L_1=[0,0,1]$$ <br>\n",
        "$$L_2=[0,1,0]$$ <br>\n",
        "$$L_3=[1,0,0]$$ <br>\n",
        "$$L_4=[0,1,0]$$\n",
        "        \n",
        "        \n",
        "        "
      ],
      "metadata": {
        "id": "2Smao3W0Zelg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onehotcode = F.one_hot(labels)\n",
        "onehotcode"
      ],
      "metadata": {
        "id": "YMn_KR2za22K",
        "outputId": "a8917c0e-6a2d-4f14-f2f5-577981043c8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 1],\n",
              "        [0, 0, 1],\n",
              "        [0, 1, 0],\n",
              "        [1, 0, 0],\n",
              "        [0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls = onehotcode * log_fsoft\n",
        "ls"
      ],
      "metadata": {
        "id": "QWRkLylOczq_",
        "outputId": "162ba3e4-1af7-4dcf-b7de-5b3c1c42b696",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0000, -0.0000, -3.8886],\n",
              "        [-0.0000, -0.0000, -2.2532],\n",
              "        [-0.0000, -0.6196, -0.0000],\n",
              "        [-1.2865, -0.0000, -0.0000],\n",
              "        [-0.0000, -0.2273, -0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls = onehotcode * log_nsoft(dw)\n",
        "ls"
      ],
      "metadata": {
        "id": "dhorS9VFg87R",
        "outputId": "a3f6992a-fcce-46f0-936a-6346033e6aab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0000, -0.0000, -3.8886],\n",
              "        [-0.0000, -0.0000, -2.2532],\n",
              "        [-0.0000, -0.6196, -0.0000],\n",
              "        [-1.2865, -0.0000, -0.0000],\n",
              "        [-0.0000, -0.2273, -0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ag4_eJGfHys"
      },
      "source": [
        "最後一步，算出Cross-Entropy的結果（以mean的型態）：\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Cross-Entropy=-\\frac{1}{5}(L_0log(S(y0))+L_1log(S(y1))+L_2log(S(y2))+L_3log(S(y3))+L_4log(S(y4)))$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=-\\small\\frac{1}{5}[0,0,1]\\cdot\\Large [{\\small log}\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}},{\\small log}\\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}},{\\small log}\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,0,1]\\cdot\\Large [{\\small log}\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}},{\\small log}\\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}},{\\small log}\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,1,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}},{\\small log}\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}},{\\small log}\\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[1,0,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}},{\\small log}\\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}},{\\small log}\\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,1,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}},{\\small log}\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}},{\\small log}\\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}$$\n",
        "$$-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=1.6550$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "th6XUFp0fHyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80d68a5f-fdde-472a-9e01-33a60a81f072"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-3.8886, -2.2532, -0.6196, -1.2865, -0.2273]), tensor(1.6550))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "sumls = torch.sum(ls, dim=1)\n",
        "ce = -torch.mean(sumls)\n",
        "sumls, ce"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">注意 `torch.mean` 是對應 $\\frac{1}{N} \\sum_{i}$，而 `torch.sum( ,dim=1)`是對應同一筆資料（同一個row）的 label one hot code 與 log 結果 的 inner product</font>"
      ],
      "metadata": {
        "id": "aPhpubiRqitq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RIPHFV7fHyx"
      },
      "source": [
        "上述步驟，在Pytorch中可以使用torch.nn.functional.nll_loss或是torch.nn.NLLLoss，注意在Pytroch裡，實際標籤$L_i$或是$Y_i$是不需要使用one-hot encoded label，所以$L$可以直接表示成一個向量[2,2,1,0,1]，向量的index就是資料的編號$i$，向量中的值就是`類`的標籤。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "Q-65RqyyfHy0",
        "outputId": "b0e5de65-14e1-4f79-cca6-072b84abe873",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# torch.nn.functional.nll_loss\n",
        "nll_out = F.nll_loss(log_fsoft, labels)\n",
        "nll_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NdVydsUxfHy2",
        "outputId": "c344f03b-55fd-49cc-9eb0-1b389d5b1d3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# torch.nn.functional.nll_loss\n",
        "NLLLoss_out = torch.nn.NLLLoss()\n",
        "NLLLoss_out(log_nsoft(dw),labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAtAl5IXfHy5"
      },
      "source": [
        "實際上，上述步驟在Pytorch中，有了權重向量（dw）與已知的分類標籤（labels），只需要一個步驟，使用`torch.nn.functional.cross_entropy`或是`torch.nn.CrossEntropyLoss`，就可求出Cross-Entroy的值"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiQ0JZ-XfHy6",
        "outputId": "e2290917-57be-437f-f919-ef2809d386a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels)\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kES_RMMfHy8",
        "outputId": "50a81939-5e84-4c6f-ae43-a9baae10c6b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss()\n",
        "CrossEntropyLoss(dw,labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTV44rfQfHy-"
      },
      "source": [
        "## 注意\n",
        "在torch.nn.functional.cross_entropy或是torch.nn.CrossEntropyLoss沒有加上reduction參數，則預設reduction='mean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "b15Qe4CrfHy_",
        "outputId": "0cfa9942-864a-4894-d99b-b883bb42fe2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels, reduction='mean')\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "g29P8AZofHzB",
        "outputId": "621eb396-c71f-42a6-f3e0-eb27a222fd9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "CrossEntropyLoss(dw, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfgAqyalfHzD"
      },
      "source": [
        "reduction='sum'表示求總和，reduction='none'表示呈現每一筆traning data的cross entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "EulJTv8LfHzF",
        "outputId": "a22183f1-a045-4b10-de18-0a290160e3f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.8886, 2.2532, 0.6196, 1.2865, 0.2273])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels, reduction='none')\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "1XLIGygIfHzH",
        "outputId": "857f2a47-2627-4b8c-eafa-86dddcd9ca7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3.8886, 2.2532, 0.6196, 1.2865, 0.2273])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "CrossEntropyLoss(dw, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ3sOeRDfHzI"
      },
      "source": [
        "此圖(圖三)乍看之下，當$N=1$時，$\\hat Y=[0.1, 0.5, 0.4]$且$Y=[0,1,0]$，似乎很好理解，但當$N>1$，容易有誤解，$N$是batch size，不是number of class，所以用$N>1$的範例來解釋圖三，$\\hat Y$與$Y$不是只有一個向量，而是$N\\times C$的矩陣，所以圖三中$[0.1, 0.5, 0.4]$是$\\hat Y$中某一個row向量，而$Y$原本是$N\\times 1$的類標籤(lable encoding)的一維向量，用one-hot encoding將一維向量，轉成$N\\times C$的2維陣列，圖三中$[0, 1, 0]$其實是$Y$中某一個row向量，圖三的$j$是batch的index而不是當成class的index，為了更清楚說明，用$i$當成batch的index，$j$換成class的index。圖三的equation可以更清楚地表示成：\n",
        "$$D(\\hat Y, Y)=-\\sum_{i=0}^{N-1}\\sum_{j=0}^{C-1} y_{ij}ln(\\hat y_{ij})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensions greater than **2** (補充)\n",
        "\n",
        "* 前一章節，**Cross Entropy**實驗用的data維度是**2**，`F.softmax`或`nn.Softmax`是基於`dim=1`。\n",
        "* 本節示範當實驗用的data維度是**3**時，執行`F.softmax`或`nn.Softmax`還是（同樣）基於`dim=1`，(而非`dim=2`)。"
      ],
      "metadata": {
        "id": "yPqc6if905_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pPCn34KwEKFm",
        "outputId": "a2129ce9-3066-4027-8b6c-b840ea6ad1cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.1258, -1.1524, -0.2506, -0.4339],\n",
              "         [ 0.8487,  0.6920, -0.3160, -2.1152]],\n",
              "\n",
              "        [[ 0.4681, -0.1577,  1.4437,  0.2660],\n",
              "         [ 0.1665,  0.8744, -0.1435, -0.1116]],\n",
              "\n",
              "        [[ 0.9318,  1.2590,  2.0050,  0.0537],\n",
              "         [ 0.6181, -0.4128, -0.8411, -2.3160]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"\n",
        "若(case 1)\n",
        "Letters=3\n",
        "Samples=2\n",
        "C=4\n",
        "而非(case 2)\n",
        "N=3\n",
        "C=2\n",
        "d1=4\n",
        "\"\"\"\n",
        "torch.manual_seed(0)\n",
        "dw = torch.randn(3, 2, 4)\n",
        "dw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.tensor([[[0.,1.,0.,0.],[0.,0.,1.,0.]],\n",
        "             [[1.,0.,0.,0.],[0.,0.,0.,1.]],\n",
        "             [[0.,0.,1.,0.],[0.,1.,0.,0.]]])\n",
        "labels"
      ],
      "metadata": {
        "id": "zwN-gmFt3WX0",
        "outputId": "0025e5b5-104a-45ee-96f3-208c3a7695fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 1., 0., 0.],\n",
              "         [0., 0., 1., 0.]],\n",
              "\n",
              "        [[1., 0., 0., 0.],\n",
              "         [0., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 1., 0.],\n",
              "         [0., 1., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.cross_entropy(dw, labels), nn.CrossEntropyLoss()(dw, labels)"
      ],
      "metadata": {
        "id": "NGgrxXLR_ja5",
        "outputId": "7e2fe227-cf1e-4947-cac0-c7c622e3c488",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5059), tensor(0.5059))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 1\n",
        "\n",
        "**不**能直接使用`F.cross_entropy`或`nn.CrossEntropyLoss`"
      ],
      "metadata": {
        "id": "WkL6knQjFavy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_sum0 = []\n",
        "ce_sum1 = []\n",
        "for letter, label in zip(dw, labels):\n",
        "  fs = F.softmax(letter, dim=1)\n",
        "  ylogyh = label*torch.log(fs)\n",
        "  print(ylogyh)\n",
        "  sumlog0 = torch.sum(ylogyh, dim=0)\n",
        "  sumlog1 = torch.sum(ylogyh, dim=1)\n",
        "  ce_sum0.append(sumlog0)\n",
        "  ce_sum1.append(sumlog1)\n",
        "torch.stack(ce_sum0), -torch.mean(torch.stack(ce_sum0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9J7uLyEFaHj",
        "outputId": "0027479d-bbb6-49a4-eede-8122aa4a0f8d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0000, -1.8783, -0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000, -1.9616, -0.0000]])\n",
            "tensor([[-1.6103, -0.0000, -0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000, -0.0000, -1.7867]])\n",
            "tensor([[-0.0000, -0.0000, -0.6721, -0.0000],\n",
            "        [-0.0000, -1.5270, -0.0000, -0.0000]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000, -1.8783, -1.9616,  0.0000],\n",
              "         [-1.6103,  0.0000,  0.0000, -1.7867],\n",
              "         [ 0.0000, -1.5270, -0.6721,  0.0000]]),\n",
              " tensor(0.7863))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.stack(ce_sum1), -torch.mean(torch.stack(ce_sum1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ers19jvj9B8",
        "outputId": "099d1c5a-bb60-49d6-adee-cc0e228f8a43"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.8783, -1.9616],\n",
              "         [-1.6103, -1.7867],\n",
              "         [-0.6721, -1.5270]]),\n",
              " tensor(1.5727))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 2\n",
        "\n",
        "能直接使用`F.cross_entropy`或`nn.CrossEntropyLoss`"
      ],
      "metadata": {
        "id": "HmKuI-t6JO1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce = F.cross_entropy(dw, labels)\n",
        "cen = F.cross_entropy(dw, labels, reduction='none')\n",
        "cen, ce"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOuQ4j_BJSmS",
        "outputId": "1809f29c-2f4b-4a34-e600-98e84e7202ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0000, 1.9912, 0.7264, -0.0000],\n",
              "         [0.5537, -0.0000, -0.0000, 0.8997],\n",
              "         [-0.0000, 1.8440, 0.0564, -0.0000]]),\n",
              " tensor(0.5059))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "手動計算 Cross Entropy"
      ],
      "metadata": {
        "id": "HKgSxbEHKe3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fsoft = F.softmax(dw, dim=1)\n",
        "fsoft"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud4liPePJuQC",
        "outputId": "df00e375-409c-4edf-ac62-9a44c8d235e3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1219, 0.1365, 0.5164, 0.8431],\n",
              "         [0.8781, 0.8635, 0.4836, 0.1569]],\n",
              "\n",
              "        [[0.5748, 0.2627, 0.8302, 0.5933],\n",
              "         [0.4252, 0.7373, 0.1698, 0.4067]],\n",
              "\n",
              "        [[0.5778, 0.8418, 0.9451, 0.9145],\n",
              "         [0.4222, 0.1582, 0.0549, 0.0855]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ylogyh = labels * torch.log(fsoft)\n",
        "sumlog = torch.sum(ylogyh, dim=1)\n",
        "cem = -torch.mean(sumlog)\n",
        "sumlog, cem"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzW__KT7LEmA",
        "outputId": "a652ef4d-bf42-4a29-fc42-50d8422b6fc9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000, -1.9912, -0.7264,  0.0000],\n",
              "         [-0.5537,  0.0000,  0.0000, -0.8997],\n",
              "         [ 0.0000, -1.8440, -0.0564,  0.0000]]),\n",
              " tensor(0.5059))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "swThxLjAzRXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "edY4g-I27KkS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}