{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrykohl/Machine-Learning-demo-repo/blob/master/jovian/cross-entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlb2kNxnfHw-"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">Cross-Entropy 在資料分類上，有著非常重要的功能。網路上關於在Pytorch中，如何使用封裝好的函數，獲得Cross Entropy的文章相當多，有的過於簡單，有的又過於複雜，深入淺出難易適中，還能從定義聯繫到實例示範（例如，手動計算Cross Entropy與利用Pytorch所獲得的結果，並比較兩結果）的文章更是不多。本文將從Cross Entropy的定義開始，列出出常看到的定義（舉兩個看起來不同的來說明），接著將用一個實際例子，先依據理論手動計算Cross Entropy，然後使用Pytorch，利用不同的封裝函數，示範如何求得Cross Entropy，再互相比較驗證此兩結果。</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feSBNrbTfHxj"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">定義一：$D(\\hat Y, Y)$與定義二：$D(S, L)$ ，兩者的差異，在於前者是mean，而後者是sum，$N$是輸入資料的筆數(也就是batch size)，$\\hat Y_i$是指第$i$筆資料的預測機率分布向量（i.e., $i$是此筆資料的index），例如第$i$筆資料的預測機率分布$\\hat Y_i$是$[0.7, 0.2, 0.1]$，此向量的維數是3，表示number of classes：$C$=3（e.g., 0表示第一類，1表示第二類，2表示第三類），$Y_i$是第$i$筆資料的實際類別標籤（e.g.,若$Y_i$的實際類別為\"0\"，那就是第一類），$Y_i$的one-hot encoded label被表示成：$[1,0,0]$。  \n",
        "再看定義二，$S$就是sample（有另一含意softmax之後會談到）, $S_i$是第$i$筆資料的預測機率分布向量（如同$\\hat Y_i$），$L$就是label，$L_i$是第$i$筆資料的實際類別標籤（如同$Y_i$）。注意，定義二中的 <font color=\"red\">$y$</font>，$S(y)$是假設只有一筆資料，如果有多筆資料，應該要表示成$S(y_i)$，$S(y_i)$才等於$Y_i$，之後用實例說明會更為清楚，$y_i$是第$i$筆資料的類模型權重向量（向量中的值可能有負值，可能大於零，也可能小於零）。</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEQmqzzBfHxp"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">在分類問題中，通過訓練logistic regression model的過程，對每一個training data，我們會得到此data的類權重分布向量，也就是 <font color=\"red\">$y$</font>，現在舉一實例，假設現在的training data set(dw)，包含五筆的權重分布向量，$N$=5，number of classes是3，$C$=3，現在用pytorcht隨機產生。</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E3VjDNP6fHxv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tz8zUZbtfHyA",
        "outputId": "ccd825ff-1a2b-4f74-b81c-7002b73aa218"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 1.5410, -0.2934, -2.1788],\n",
              "         [ 0.5684, -1.0845, -1.3986],\n",
              "         [ 0.4033,  0.8380, -0.7193],\n",
              "         [-0.4033, -0.5966,  0.1820],\n",
              "         [-0.8567,  1.1006, -1.0712]]),\n",
              " torch.Size([5, 3]))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.manual_seed(0)\n",
        "dw = torch.randn(5, 3)\n",
        "dw, dw.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai9-JYjESkSR",
        "outputId": "9fe07d32-78db-4998-9718-2d97ea52d559"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([2, 2, 1, 0, 1])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = torch.tensor([2,2,1,0,1])\n",
        "labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKBeB4o4fHyN"
      },
      "source": [
        "dw.shape，就是（$N$, $C$），有5筆權重向量或是Scores/Logits向量，類別數為3，第一筆 <font color=\"red\">$y$</font> 就是dw[0]，第二筆 <font color=\"red\">$y$</font> 就是dw[1]，...。在權重向量中，有正值有負值，可能有大於1，也可能小於1。因此權重向量需要透過softmax處理，轉成機率分布向量。softmax的定義如下：\n",
        "\n",
        "<br>\n",
        "\n",
        "$S(y_i)=\\Large \\frac{e^{y_{ij}}}{\\sum_{j}^{C} e^{y_{ij}}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$y_{ij}$ 是指第$i$筆權重向量中的第$j$個的值。根據softmax的定義，可以算出$S(y_i)$，$0\\leq i< N=5$\n",
        "\n",
        "<br>\n",
        "\n",
        "$S(y_0)=\\Large [\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, \\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, \\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$\n",
        "\n",
        "\n",
        "$S(y_1)=\\Large [\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, \\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, \\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$\n",
        "\n",
        "$S(y_2)=\\Large [\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, \\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, \\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$\n",
        "\n",
        "$S(y_3)=\\Large [\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, \\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, \\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$\n",
        "\n",
        "$S(y_4)=\\Large [\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, \\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, \\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "在Pytorch中，可以用torch.nn.functional.softmax或是torch.nn.Softmax（注意大小寫）兩種方式。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmj1oPjPfHyS",
        "outputId": "4419cb4d-440c-4ea0-a596-aa296c9c3159"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.8446, 0.1349, 0.0205],\n",
              "        [0.7511, 0.1438, 0.1051],\n",
              "        [0.3484, 0.5382, 0.1134],\n",
              "        [0.2762, 0.2277, 0.4961],\n",
              "        [0.1125, 0.7967, 0.0908]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.softmax\n",
        "fsoft = F.softmax(dw, dim=1)\n",
        "fsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkEmlD3zfHyW",
        "outputId": "b4e40104-821d-4315-fca2-1d95144d4997"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.8446, 0.1349, 0.0205],\n",
              "        [0.7511, 0.1438, 0.1051],\n",
              "        [0.3484, 0.5382, 0.1134],\n",
              "        [0.2762, 0.2277, 0.4961],\n",
              "        [0.1125, 0.7967, 0.0908]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.Softmax\n",
        "nsoft = nn.Softmax(dim=1)\n",
        "nsoft(dw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqrmlYwHfHya"
      },
      "source": [
        "fsoft與nsoft的結果完全一致。接著是很簡單的一步驟，對$S(y_i)$取$log$，注意在Pytorch中log是以$e$為基底（$log$是$ln$）：\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_0))$=$\\Large [{\\small log}\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, {\\small log}\\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, {\\small log}\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_1))$=$\\Large [{\\small log}\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, {\\small log}\\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, {\\small log}\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_2))$=$\\Large [{\\small log}\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, {\\small log}\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, {\\small log}\\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_3))$=$\\Large [{\\small log}\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, {\\small log}\\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, {\\small log}\\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_4))$=$\\Large [{\\small log}\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, {\\small log}\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, {\\small log}\\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQURbdfzfHyg",
        "outputId": "e248647c-3869-44f6-a6b0-7fc7dde9728f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logfsoft = torch.log(fsoft)\n",
        "logfsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfIMNl5FfHyj",
        "outputId": "01145754-98d9-46b4-8821-4f903f3d1388"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lognsoft = torch.log(nsoft(dw))\n",
        "lognsoft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2p9uWaAfHyl"
      },
      "source": [
        "上述兩個步驟($log$ + $softmax$)在Pytorch中可以合併成一個步驟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs8W9MASfHyn",
        "outputId": "3e7564df-7332-40bf-ac75-5a1178f4198d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional\n",
        "log_fsoft = F.log_softmax(dw, dim=1)\n",
        "log_fsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOsiSVWafHyq",
        "outputId": "6c7429d0-d983-42d8-f361-99bab6bd05e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1689, -2.0033, -3.8886],\n",
              "        [-0.2862, -1.9392, -2.2532],\n",
              "        [-1.0543, -0.6196, -2.1769],\n",
              "        [-1.2865, -1.4797, -0.7011],\n",
              "        [-2.1846, -0.2273, -2.3991]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn\n",
        "log_nsoft = nn.LogSoftmax(dim=1)\n",
        "log_nsoft(dw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Smao3W0Zelg"
      },
      "source": [
        "資料標籤 **labels**: `[2,2,1,0,1]`，下一個步驟是把 **`labels`** 與 $log(S(y))$ ，執行 Element-wise multiplication & Summation, 也就是 Inner product.\n",
        "\n",
        "假設 $L_0$ 是 **2**, $L_1$ 是 **2**, $L_2$ 是 **1**, $L_3$ 是 **0**, $L_4$ 是 **1**，對應的one-hot encode 樣態：\n",
        "\n",
        "<br>\n",
        "\n",
        "$$L_0=[0,0,1]$$ <br>\n",
        "$$L_1=[0,0,1]$$ <br>\n",
        "$$L_2=[0,1,0]$$ <br>\n",
        "$$L_3=[1,0,0]$$ <br>\n",
        "$$L_4=[0,1,0]$$\n",
        "        \n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMn_KR2za22K",
        "outputId": "fd57170d-5260-4f2c-a584-178c8e63ff7d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0, 0, 1],\n",
              "        [0, 0, 1],\n",
              "        [0, 1, 0],\n",
              "        [1, 0, 0],\n",
              "        [0, 1, 0]])"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "onehotcode = F.one_hot(labels, num_classes=3)\n",
        "onehotcode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWRkLylOczq_",
        "outputId": "e0648903-67ff-4f8d-bee0-027aaf6a4e3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0000, -0.0000, -3.8886],\n",
              "        [-0.0000, -0.0000, -2.2532],\n",
              "        [-0.0000, -0.6196, -0.0000],\n",
              "        [-1.2865, -0.0000, -0.0000],\n",
              "        [-0.0000, -0.2273, -0.0000]])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 根據 F.log_softmax\n",
        "ls = onehotcode * log_fsoft\n",
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhorS9VFg87R",
        "outputId": "228170d6-f925-4c3f-a696-5fcf0719e76d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0000, -0.0000, -3.8886],\n",
              "        [-0.0000, -0.0000, -2.2532],\n",
              "        [-0.0000, -0.6196, -0.0000],\n",
              "        [-1.2865, -0.0000, -0.0000],\n",
              "        [-0.0000, -0.2273, -0.0000]])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 根據 nn.LogSoftmax\n",
        "ls = onehotcode * log_nsoft(dw)\n",
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ag4_eJGfHys"
      },
      "source": [
        "最後一步，算出Cross-Entropy的結果（求 `mean` & `取負`）：\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Cross-Entropy=-\\frac{1}{N}\\sum_{i} L_i*log(S(y_i))$$\n",
        "\n",
        "$$=-\\frac{1}{5}(L_0log(S(y_0))+L_1log(S(y_1))+L_2log(S(y_2))+L_3log(S(y_3))+L_4log(S(y_4)))$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=-\\small\\frac{1}{5}[0,0,1]\\cdot\\Large [{\\small log}\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}},{\\small log}\\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}},{\\small log}\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,0,1]\\cdot\\Large [{\\small log}\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}},{\\small log}\\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}},{\\small log}\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,1,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}},{\\small log}\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}},{\\small log}\\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[1,0,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}},{\\small log}\\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}},{\\small log}\\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,1,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}},{\\small log}\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}},{\\small log}\\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}$$\n",
        "$$-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=1.6550$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th6XUFp0fHyy",
        "outputId": "4e0551e5-448f-4185-f01c-abc6584ed285"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([-3.8886, -2.2532, -0.6196, -1.2865, -0.2273]), tensor(1.6550))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sumls = torch.sum(ls, dim=1)\n",
        "ce = -torch.mean(sumls)\n",
        "sumls, ce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPhpubiRqitq"
      },
      "source": [
        "<font color=\"red\">注意 `torch.mean` 是對應 $\\frac{1}{N} \\sum_{i}$，而 `torch.sum( ,dim=1)`是對應同一筆資料`y`的 label one hot code 與 log(S(`y`))  的 inner product 中 `sum` 作用</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RIPHFV7fHyx"
      },
      "source": [
        "$log(S(y))$之後的步驟，在Pytorch中可以使用`torch.nn.functional.nll_loss`或是`torch.nn.NLLLoss`，注意在Pytroch裡，實際標籤$L_i$是不需要使用one-hot encoded 樣式，所以**labels**可以直接表示成一個向量[2,2,1,0,1]，向量的index就是資料的編號$i$，向量中的值就是`類`的標籤。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-65RqyyfHy0",
        "outputId": "b0e5de65-14e1-4f79-cca6-072b84abe873"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.nll_loss\n",
        "nll_out = F.nll_loss(log_fsoft, labels)\n",
        "nll_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdVydsUxfHy2",
        "outputId": "c344f03b-55fd-49cc-9eb0-1b389d5b1d3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.nll_loss\n",
        "NLLLoss_out = torch.nn.NLLLoss()\n",
        "NLLLoss_out(log_nsoft(dw),labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAtAl5IXfHy5"
      },
      "source": [
        "實際上，上述步驟在Pytorch中，有了權重向量（dw）與分類標籤（labels），只需要一個步驟，使用`torch.nn.functional.cross_entropy`或是`torch.nn.CrossEntropyLoss`，就可求出Cross-Entroy的值"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiQ0JZ-XfHy6",
        "outputId": "3e6d2013-e319-4573-9621-d6de02166fed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels)\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kES_RMMfHy8",
        "outputId": "d2594a99-dec1-4f27-81a4-3a9a5dbe3b9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss()\n",
        "CrossEntropyLoss(dw,labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTV44rfQfHy-"
      },
      "source": [
        "注意，在torch.nn.functional.cross_entropy或是torch.nn.CrossEntropyLoss沒有加上reduction參數，則預設reduction='mean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b15Qe4CrfHy_",
        "outputId": "0cfa9942-864a-4894-d99b-b883bb42fe2b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels, reduction='mean')\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g29P8AZofHzB",
        "outputId": "621eb396-c71f-42a6-f3e0-eb27a222fd9a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.6550)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "CrossEntropyLoss(dw, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfgAqyalfHzD"
      },
      "source": [
        "reduction='sum'表示求總和，reduction='none'表示呈現每一筆traning data的cross entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EulJTv8LfHzF",
        "outputId": "a22183f1-a045-4b10-de18-0a290160e3f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.8886, 2.2532, 0.6196, 1.2865, 0.2273])"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels, reduction='none')\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XLIGygIfHzH",
        "outputId": "857f2a47-2627-4b8c-eafa-86dddcd9ca7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([3.8886, 2.2532, 0.6196, 1.2865, 0.2273])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "CrossEntropyLoss(dw, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ3sOeRDfHzI"
      },
      "source": [
        "此圖(圖三)乍看之下，當$N=1$時，$\\hat Y=[0.1, 0.5, 0.4]$且$Y=[0,1,0]$，似乎很好理解，但當$N>1$，容易有誤解，$N$是batch size，不是number of class，所以用$N>1$的範例來解釋圖三，$\\hat Y$與$Y$不是只有一個向量，而是$N\\times C$的矩陣，所以圖三中$[0.1, 0.5, 0.4]$是$\\hat Y$中某一個row向量，而$Y$原本是$N\\times 1$的類標籤(lable encoding)的一維向量，用one-hot encoding將一維向量，轉成$N\\times C$的2維陣列，圖三中$[0, 1, 0]$其實是$Y$中某一個row向量，圖三的$j$是batch的index而不是當成class的index，為了更清楚說明，用$i$當成batch的index，$j$換成class的index。圖三的equation可以更清楚地表示成：\n",
        "$$D(\\hat Y, Y)=-\\sum_{i=0}^{N-1}\\sum_{j=0}^{C-1} y_{ij}ln(\\hat y_{ij})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPqc6if905_-"
      },
      "source": [
        "# Dimensions greater than **2** (補充)\n",
        "\n",
        "* 前一章節，**Cross Entropy**實驗用的data維度是**2**，`F.softmax`或`nn.Softmax`是基於`dim=1`。\n",
        "* 本節示範當實驗用的data維度是**3**時，執行`F.softmax`或`nn.Softmax`還是（同樣）基於`dim=1`，(而非`dim=2`)。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPCn34KwEKFm",
        "outputId": "e5b185c7-1d09-434d-d235-5e9789bc3f93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-1.1258, -1.1524, -0.2506, -0.4339],\n",
              "         [ 0.8487,  0.6920, -0.3160, -2.1152]],\n",
              "\n",
              "        [[ 0.4681, -0.1577,  1.4437,  0.2660],\n",
              "         [ 0.1665,  0.8744, -0.1435, -0.1116]],\n",
              "\n",
              "        [[ 0.9318,  1.2590,  2.0050,  0.0537],\n",
              "         [ 0.6181, -0.4128, -0.8411, -2.3160]]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "若(case 1)\n",
        "Letters=3\n",
        "Samples=2\n",
        "C=4\n",
        "而非(case 2)\n",
        "N=3\n",
        "C=2\n",
        "d1=4\n",
        "\"\"\"\n",
        "torch.manual_seed(0)\n",
        "dw = torch.randn(3, 2, 4)\n",
        "dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwN-gmFt3WX0",
        "outputId": "ae6e1dd2-1b6d-41da-cbf5-3fd18e57c2b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0, 1, 0, 0],\n",
              "         [0, 0, 1, 0]],\n",
              "\n",
              "        [[1, 0, 0, 0],\n",
              "         [0, 0, 0, 1]],\n",
              "\n",
              "        [[0, 0, 1, 0],\n",
              "         [0, 1, 0, 0]]])"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = torch.tensor([[1,2],[0,3],[2,1]])\n",
        "\n",
        "labelscode = F.one_hot(labels)\n",
        "labelscode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wii7ZIQBkmK",
        "outputId": "7ba0bd4c-95a6-4172-d46c-c60dbb5bd7fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.5059), tensor(0.5059))"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F.cross_entropy(dw, labelscode.float()), nn.CrossEntropyLoss()(dw, labelscode.float())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEGoFljJKa7h"
      },
      "source": [
        "<font color=\"red\">計算 Cross entropy時，`dw`的維度超過**2**，`labels`(未做one hot coding之前)的維度超過**1**，`labelscode`(經one hot coding 轉換)需要加上`.float()`，否則上一行執行時會出錯</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkL6knQjFavy"
      },
      "source": [
        "## CASE 1\n",
        "\n",
        "<font color=\"blue\">**不**能直接使用`F.cross_entropy`或`nn.CrossEntropyLoss`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbmJ1q2CwtYK"
      },
      "source": [
        "<font color=\"blue\">完全不使用`F.cross_entropy`</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9J7uLyEFaHj",
        "outputId": "a712a42d-f50b-4adf-a83d-95f24336a1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0000, -1.8783, -0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000, -1.9616, -0.0000]])\n",
            "tensor([[-1.6103, -0.0000, -0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000, -0.0000, -1.7867]])\n",
            "tensor([[-0.0000, -0.0000, -0.6721, -0.0000],\n",
            "        [-0.0000, -1.5270, -0.0000, -0.0000]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([[-1.8783, -1.9616],\n",
              "         [-1.6103, -1.7867],\n",
              "         [-0.6721, -1.5270]]),\n",
              " tensor(1.5727))"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ce_sum = []\n",
        "for letter, label in zip(dw, labels):\n",
        "  fs = F.softmax(letter, dim=1)\n",
        "  ylogyh = F.one_hot(label, num_classes=4)*torch.log(fs)\n",
        "  print(ylogyh)\n",
        "\n",
        "  sumlog = torch.sum(ylogyh, dim=1)\n",
        "  ce_sum.append(sumlog)\n",
        "torch.stack(ce_sum), -torch.mean(torch.stack(ce_sum))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyyn8xyrxE13"
      },
      "source": [
        "<font color=\"blue\">使用`F.cross_entropy`（需將資料拆解後，分批使用，再合併結果）</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QytCcXphuLNe",
        "outputId": "2f33e5b3-f562-432b-c7dd-452830c1c3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.8783, 1.9616],\n",
            "        [1.6103, 1.7867],\n",
            "        [0.6721, 1.5270]])\n",
            "tensor(1.5727)\n"
          ]
        }
      ],
      "source": [
        "ce_sum = []\n",
        "for letter, label in zip(dw, labels):\n",
        "  fs = F.softmax(letter, dim=1)\n",
        "  ce_sum.append(F.cross_entropy(letter, label, reduction='none'))\n",
        "\n",
        "print(torch.stack(ce_sum))\n",
        "print(torch.mean(torch.stack(ce_sum))) # 不需要取負號，因為F.cross_entropy已經算入負號"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmKuI-t6JO1e"
      },
      "source": [
        "## CASE 2\n",
        "\n",
        "<font color=\"blue\">能直接使用`F.cross_entropy`或`nn.CrossEntropyLoss`</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOuQ4j_BJSmS",
        "outputId": "220e088e-7021-4e0c-e2ec-b4379a24d4d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.0000, 1.9912, 0.7264, -0.0000],\n",
              "         [0.5537, -0.0000, -0.0000, 0.8997],\n",
              "         [-0.0000, 1.8440, 0.0564, -0.0000]]),\n",
              " tensor(0.5059))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cen = F.cross_entropy(dw, labelscode.float(), reduction='none')\n",
        "ce = F.cross_entropy(dw, labelscode.float())\n",
        "cen, ce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKgSxbEHKe3N"
      },
      "source": [
        "<font color=\"blue\">手動計算 Cross Entropy</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud4liPePJuQC",
        "outputId": "0a627ad7-eecb-4a14-dd7b-f5a10c0dde1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.1219, 0.1365, 0.5164, 0.8431],\n",
              "         [0.8781, 0.8635, 0.4836, 0.1569]],\n",
              "\n",
              "        [[0.5748, 0.2627, 0.8302, 0.5933],\n",
              "         [0.4252, 0.7373, 0.1698, 0.4067]],\n",
              "\n",
              "        [[0.5778, 0.8418, 0.9451, 0.9145],\n",
              "         [0.4222, 0.1582, 0.0549, 0.0855]]])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fsoft = F.softmax(dw, dim=1)\n",
        "fsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzW__KT7LEmA",
        "outputId": "e3cb371c-0395-4984-b3ae-29cc8f670536"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000, -1.9912, -0.7264,  0.0000],\n",
              "         [-0.5537,  0.0000,  0.0000, -0.8997],\n",
              "         [ 0.0000, -1.8440, -0.0564,  0.0000]]),\n",
              " tensor(0.5059))"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ylogyh = labelscode * torch.log(fsoft)\n",
        "sumlog = torch.sum(ylogyh, dim=1) ## 此sum是inner product的部分，針對同筆資料不同類的加總\n",
        "cem = -torch.mean(sumlog)\n",
        "sumlog, cem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Another case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edY4g-I27KkS"
      },
      "outputs": [],
      "source": [
        "cel = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ap = torch.randn(2,4) ## 兩筆資料\n",
        "sp = F.softmax(ap, dim=1) ## 模擬模型輸出的數據\n",
        "sp "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lp = torch.randint(4,(2,)) ## 模擬資料真實標籤\n",
        "num_class = 2\n",
        "lph=torch.zeros(2, 4).scatter_(1, lp.long().reshape(-1, 1), 1) ## one-hot 轉換 (long() 非必需)\n",
        "lp, lph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* lp 若要手動設定，需使用 torch.tensor，不能使用 torch.Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cel(sp, lp), cel(sp, lph)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
