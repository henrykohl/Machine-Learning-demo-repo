{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrykohl/Machine-Learning-demo-repo/blob/master/jovian/cross-entropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qlb2kNxnfHw-"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">Cross-Entropy 在資料分類上，有著非常重要的功能。網路上關於在Pytorch中，如何使用封裝好的函數，獲得Cross Entropy的文章相當多，有的過於簡單，有的又過於複雜，深入淺出難易適中，還能從定義聯繫到實例示範（例如，手動計算Cross Entropy與利用Pytorch所獲得的結果，並比較兩結果）的文章更是不多。本文將從Cross Entropy的定義開始，列出出常看到的定義（舉兩個看起來不同的來說明），接著將用一個實際例子，先依據理論手動計算Cross Entropy，然後使用Pytorch，利用不同的封裝函數，示範如何求得Cross Entropy，再互相比較驗證此兩結果。</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feSBNrbTfHxj"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">定義一：$D(\\hat Y, Y)$與定義二：$D(S, L)$ ，兩者的差異，在於前者是mean，而後者是sum，$N$是輸入資料的筆數(也就是batch size)，$\\hat Y_i$是指第$i$筆資料的預測機率分布向量（i.e., $i$是此筆資料的index），例如第$i$筆資料的預測機率分布$\\hat Y_i$是$[0.7, 0.2, 0.1]$，此向量的維數是3，表示number of classes：$C$=3（e.g., 0表示第一類，1表示第二類，2表示第三類），$Y_i$是第$i$筆資料的實際類別標籤（e.g.,若$Y_i$的實際類別為\"0\"，那就是第一類），$Y_i$的one-hot encoded label被表示成：$[1,0,0]$。  \n",
        "再看定義二，$S$就是sample（有另一含意softmax之後會談到）, $S_i$是第$i$筆資料的預測機率分布向量（如同$\\hat Y_i$），$L$就是label，$L_i$是第$i$筆資料的實際類別標籤（如同$Y_i$）。注意，定義二中的 <font color=\"red\">$y$</font>，$S(y)$是假設只有一筆資料，如果有多筆資料，應該要表示成$S(y_i)$，$S(y_i)$才等於$Y_i$，之後用實例說明會更為清楚，$y_i$是第$i$筆資料的類模型權重向量（向量中的值可能有負值，可能大於零，也可能小於零）。</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEQmqzzBfHxp"
      },
      "source": [
        "<span style=\"font-size:14px; font-family:Arial;\">在分類問題中，通過訓練logistic regression model的過程，對每一個training data，我們會得到此data的類權重分布向量，也就是 <font color=\"red\">$y$</font>，現在舉一實例，假設現在的training data set(dw)，包含五筆的權重分布向量，$N$=5，number of classes是3，$C$=3，現在用pytorcht隨機產生。</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E3VjDNP6fHxv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz8zUZbtfHyA",
        "outputId": "bdd43832-b526-4af0-fb5b-c89436a801e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.7085, -0.6024, -0.4136],\n",
              "        [ 0.5424,  0.2090,  1.0417],\n",
              "        [ 0.1108,  0.5909, -0.6762],\n",
              "        [ 1.0043, -0.0084, -1.3112],\n",
              "        [-0.6292,  0.7759, -1.5004]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dw = torch.randn(5, 3)\n",
        "dw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YOz4GmWfHyK",
        "outputId": "78d7224b-13f7-4f6a-9641-9977ad547446"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dw.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKBeB4o4fHyN"
      },
      "source": [
        "dw.shape，就是（$N$, $C$），有5筆權重向量或是Scores/Logits向量，類別數為3，第一筆 <font color=\"red\">$y$</font> 就是dw[0]，第二筆 <font color=\"red\">$y$</font> 就是dw[1]，...。在權重向量中，有正值有負值，可能有大於1，也可能小於1。因此權重向量需要透過softmax處理，轉成機率分布向量。softmax的定義如下：\n",
        "\n",
        "<br>\n",
        "\n",
        "$S(y_i)=\\Large \\frac{e^{y_{ij}}}{\\sum_{j}^{C} e^{y_{ij}}}$\n",
        "\n",
        "<br>\n",
        "\n",
        "$y_{ij}$ 是指第$i$筆權重向量中的第$j$個的值。根據softmax的定義，可以算出$S(y_i)$，$0\\leq i< N=5$\n",
        "\n",
        "<br>\n",
        "\n",
        "$S(y_0)=\\Large [\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, \\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, \\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$\n",
        "\n",
        "\n",
        "$S(y_1)=\\Large [\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, \\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, \\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$\n",
        "\n",
        "$S(y_2)=\\Large [\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, \\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, \\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$\n",
        "\n",
        "$S(y_3)=\\Large [\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, \\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, \\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$\n",
        "\n",
        "$S(y_4)=\\Large [\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, \\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, \\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "在Pytorch中，可以用torch.nn.functional.softmax或是torch.nn.Softmax（注意大小寫）兩種方式。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vmj1oPjPfHyS",
        "outputId": "c1292451-e68a-458f-f307-b7cb2defac79"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.2894, 0.3218, 0.3887],\n",
              "        [0.2973, 0.2130, 0.4898],\n",
              "        [0.3256, 0.5262, 0.1482],\n",
              "        [0.6840, 0.2485, 0.0675],\n",
              "        [0.1820, 0.7418, 0.0762]])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.softmax\n",
        "fsoft = F.softmax(dw, dim=1)\n",
        "fsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkEmlD3zfHyW",
        "outputId": "fff73cee-c8a7-476c-ecdb-83aa3b115dae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.2894, 0.3218, 0.3887],\n",
              "        [0.2973, 0.2130, 0.4898],\n",
              "        [0.3256, 0.5262, 0.1482],\n",
              "        [0.6840, 0.2485, 0.0675],\n",
              "        [0.1820, 0.7418, 0.0762]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.Softmax\n",
        "nsoft = nn.Softmax(dim=1)\n",
        "nsoft(dw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqrmlYwHfHya"
      },
      "source": [
        "fsoft與nsoft的結果完全一致。接著是很簡單的一步驟，對$S(y_i)$取log，注意在Pytorch中log是以$e$為基底（log是ln）：\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_0))$=$\\Large [{\\small log}\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, {\\small log}\\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}, {\\small log}\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_1))$=$\\Large [{\\small log}\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, {\\small log}\\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}, {\\small log}\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_2))$=$\\Large [{\\small log}\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, {\\small log}\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}, {\\small log}\\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_3))$=$\\Large [{\\small log}\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, {\\small log}\\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}, {\\small log}\\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$\n",
        "\n",
        "<br>\n",
        "\n",
        "$log(S(y_4))$=$\\Large [{\\small log}\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, {\\small log}\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}, {\\small log}\\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQURbdfzfHyg",
        "outputId": "21d91863-0cb7-456b-e75b-8c6bdb9ecb13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.2398, -1.1337, -0.9449],\n",
              "        [-1.2132, -1.5465, -0.7139],\n",
              "        [-1.1222, -0.6420, -1.9091],\n",
              "        [-0.3798, -1.3924, -2.6953],\n",
              "        [-1.7037, -0.2986, -2.5749]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logfsoft = torch.log(fsoft)\n",
        "logfsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfIMNl5FfHyj",
        "outputId": "7b039451-e530-40a1-ac08-4d339159f1ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.2398, -1.1337, -0.9449],\n",
              "        [-1.2132, -1.5465, -0.7139],\n",
              "        [-1.1222, -0.6420, -1.9091],\n",
              "        [-0.3798, -1.3924, -2.6953],\n",
              "        [-1.7037, -0.2986, -2.5749]])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lognsoft = torch.log(nsoft(dw))\n",
        "lognsoft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2p9uWaAfHyl"
      },
      "source": [
        "上述兩個步驟在Pytorch中可以合併成一個步驟"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gs8W9MASfHyn",
        "outputId": "750f6ec2-4fab-4a3e-e27c-ca751b77f30d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.2398, -1.1337, -0.9449],\n",
              "        [-1.2132, -1.5465, -0.7139],\n",
              "        [-1.1222, -0.6420, -1.9091],\n",
              "        [-0.3798, -1.3924, -2.6953],\n",
              "        [-1.7037, -0.2986, -2.5749]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional\n",
        "log_fsoft = F.log_softmax(dw, dim=1)\n",
        "log_fsoft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOsiSVWafHyq",
        "outputId": "8541da34-1e24-4e6f-e844-7126725507ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.2398, -1.1337, -0.9449],\n",
              "        [-1.2132, -1.5465, -0.7139],\n",
              "        [-1.1222, -0.6420, -1.9091],\n",
              "        [-0.3798, -1.3924, -2.6953],\n",
              "        [-1.7037, -0.2986, -2.5749]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn\n",
        "log_nsoft = nn.LogSoftmax(dim=1)\n",
        "log_nsoft(dw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ag4_eJGfHys"
      },
      "source": [
        "假設 $L_0$ 的 label是2, $L_1$ 的 label是2, $L_2$ 的 label是1, $L_3$ 的 label是0, $L_4$ 的 label是1，則他們的one-hot encoded labels：\n",
        "\n",
        "<br>\n",
        "\n",
        "$$L_0=[0,0,1]$$ <br>\n",
        "\n",
        "$$L_1=[0,0,1]$$ <br>\n",
        "\n",
        "$$L_2=[0,1,0]$$ <br>\n",
        "\n",
        "$$L_3=[1,0,0]$$ <br>\n",
        "\n",
        "$$L_4=[0,1,0]$$\n",
        "\n",
        "最後一步，算出Cross-Entropy的結果（以mean的型態）：\n",
        "\n",
        "<br>\n",
        "\n",
        "$$Cross-Entropy=-\\frac{1}{5}(L_0log(S(y0))+L_1log(S(y1))+L_2log(S(y2))+L_3log(S(y3))+L_4log(S(y4)))$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=-\\small\\frac{1}{5}[0,0,1]\\cdot\\Large [{\\small log}\\frac{e^{dw[0,0]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}},{\\small log}\\frac{e^{dw[0,1]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}},{\\small log}\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,0,1]\\cdot\\Large [{\\small log}\\frac{e^{dw[1,0]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}},{\\small log}\\frac{e^{dw[1,1]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}},{\\small log}\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,1,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[2,0]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}},{\\small log}\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}},{\\small log}\\frac{e^{dw[2,2]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[1,0,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}},{\\small log}\\frac{e^{dw[3,1]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}},{\\small log}\\frac{e^{dw[3,2]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}]$$\n",
        "\n",
        "$$-\\small\\frac{1}{5}[0,1,0]\\cdot\\Large [{\\small log}\\frac{e^{dw[4,0]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}},{\\small log}\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}},{\\small log}\\frac{e^{dw[4,2]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}]$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[0,2]}}{e^{dw[0,0]}{\\small +}e^{dw[0,1]}{\\small +}e^{dw[0,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[1,2]}}{e^{dw[1,0]}{\\small +}e^{dw[1,1]}{\\small +}e^{dw[1,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[2,1]}}{e^{dw[2,0]}{\\small +}e^{dw[2,1]}{\\small +}e^{dw[2,2]}}$$\n",
        "$$-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[3,0]}}{e^{dw[3,0]}{\\small +}e^{dw[3,1]}{\\small +}e^{dw[3,2]}}-\\small\\frac{1}{5}{\\small log}\\Large\\frac{e^{dw[4,1]}}{e^{dw[4,0]}{\\small +}e^{dw[4,1]}{\\small +}e^{dw[4,2]}}$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$=0.5958$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RIPHFV7fHyx"
      },
      "source": [
        "在Pytorch中，求Cross-Entropy的最後一步，是使用torch.nn.functional.nll_loss或是torch.nn.NLLLoss，注意在Pytroch裡，實際標籤$L_i$或是$Y_i$是不需要使用one-hot encoded label，所以$L$可以直接表示成一個向量[2,2,1,0,1]，向量的index就是資料的編號$i$，向量中的值就是類標籤。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th6XUFp0fHyy"
      },
      "outputs": [],
      "source": [
        "labels = torch.tensor([2,2,1,0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-65RqyyfHy0",
        "outputId": "33e6c446-43c1-470e-a30c-dce9bf7322e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.nll_loss\n",
        "nll_out = F.nll_loss(log_fsoft, labels)\n",
        "nll_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdVydsUxfHy2",
        "outputId": "2d350687-8d6a-4e22-f0ee-f43c9b501036"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.nll_loss\n",
        "NLLLoss_out = torch.nn.NLLLoss()\n",
        "NLLLoss_out(log_nsoft(dw),labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAtAl5IXfHy5"
      },
      "source": [
        "實際上，上述步驟在Pytorch中，有了權重向量（dw）與已知的分類標籤（labels），只需要一個步驟，使用`torch.nn.functional.cross_entropy`或是`torch.nn.CrossEntropyLoss`，就可求出Cross-Entroy的值"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiQ0JZ-XfHy6",
        "outputId": "e2290917-57be-437f-f919-ef2809d386a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels)\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kES_RMMfHy8",
        "outputId": "50a81939-5e84-4c6f-ae43-a9baae10c6b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss()\n",
        "CrossEntropyLoss(dw,labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTV44rfQfHy-"
      },
      "source": [
        "## 注意\n",
        "在torch.nn.functional.cross_entropy或是torch.nn.CrossEntropyLoss沒有加上reduction參數，則預設reduction='mean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b15Qe4CrfHy_",
        "outputId": "7382a16a-d122-435f-9ef7-1045bae1acb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels, reduction='mean')\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g29P8AZofHzB",
        "outputId": "88236c24-38ae-46ad-c200-22df3174d8a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5958)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss(reduction='mean')\n",
        "CrossEntropyLoss(dw, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfgAqyalfHzD"
      },
      "source": [
        "reduction='sum'表示求總和，reduction='none'表示呈現每一筆traning data的cross entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EulJTv8LfHzF",
        "outputId": "d3979655-2ebc-4a00-b43b-6fb02d2dd619"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.9449, 0.7139, 0.6420, 0.3798, 0.2986])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.functional.cross_entropy\n",
        "cross_entropy = F.cross_entropy(dw, labels, reduction='none')\n",
        "cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XLIGygIfHzH",
        "outputId": "77b1b78e-5ead-42d4-ac21-44554ddfb51f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.9449, 0.7139, 0.6420, 0.3798, 0.2986])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.nn.CrossEntropyLoss\n",
        "CrossEntropyLoss = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "CrossEntropyLoss(dw, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ3sOeRDfHzI"
      },
      "source": [
        "此圖(圖三)乍看之下，當$N=1$時，$\\hat Y=[0.1, 0.5, 0.4]$且$Y=[0,1,0]$，似乎很好理解，但當$N>1$，容易有誤解，$N$是batch size，不是number of class，所以用$N>1$的範例來解釋圖三，$\\hat Y$與$Y$不是只有一個向量，而是$N\\times C$的矩陣，所以圖三中$[0.1, 0.5, 0.4]$是$\\hat Y$中某一個row向量，而$Y$原本是$N\\times 1$的類標籤(lable encoding)的一維向量，用one-hot encoding將一維向量，轉成$N\\times C$的2維陣列，圖三中$[0, 1, 0]$其實是$Y$中某一個row向量，圖三的$j$是batch的index而不是當成class的index，為了更清楚說明，用$i$當成batch的index，$j$換成class的index。圖三的equation可以更清楚地表示成：\n",
        "$$D(\\hat Y, Y)=-\\sum_{i=0}^{N-1}\\sum_{j=0}^{C-1} y_{ij}ln(\\hat y_{ij})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dimensions greater than **2** (補充)\n",
        "\n",
        "* 前一章節，**Cross Entropy**實驗用的data維度是**2**，`F.softmax`或`nn.Softmax`是基於`dim=1`。\n",
        "* 本節示範當實驗用的data維度是**3**時，執行`F.softmax`或`nn.Softmax`還是（同樣）基於`dim=1`，(而非`dim=2`)。"
      ],
      "metadata": {
        "id": "yPqc6if905_-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pPCn34KwEKFm",
        "outputId": "190578b6-a351-47e8-8f5f-7da246d0e029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.1258, -1.1524, -0.2506, -0.4339],\n",
              "         [ 0.8487,  0.6920, -0.3160, -2.1152]],\n",
              "\n",
              "        [[ 0.4681, -0.1577,  1.4437,  0.2660],\n",
              "         [ 0.1665,  0.8744, -0.1435, -0.1116]],\n",
              "\n",
              "        [[ 0.9318,  1.2590,  2.0050,  0.0537],\n",
              "         [ 0.6181, -0.4128, -0.8411, -2.3160]]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\"\"\"\n",
        "若(case 1)\n",
        "Letters=3\n",
        "Samples=2\n",
        "C=4\n",
        "而非(case 2)\n",
        "N=3\n",
        "C=2\n",
        "d1=4\n",
        "\"\"\"\n",
        "torch.manual_seed(0)\n",
        "dw = torch.randn(3, 2, 4)\n",
        "dw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.tensor([[[0.,1.,0.,0.],[0.,0.,1.,0.]],\n",
        "             [[1.,0.,0.,0.],[0.,0.,0.,1.]],\n",
        "             [[0.,0.,1.,0.],[0.,1.,0.,0.]]])\n",
        "labels"
      ],
      "metadata": {
        "id": "zwN-gmFt3WX0",
        "outputId": "815466ef-3223-4f13-d099-50cb9cd16608",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 1., 0., 0.],\n",
              "         [0., 0., 1., 0.]],\n",
              "\n",
              "        [[1., 0., 0., 0.],\n",
              "         [0., 0., 0., 1.]],\n",
              "\n",
              "        [[0., 0., 1., 0.],\n",
              "         [0., 1., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.cross_entropy(dw, labels), nn.CrossEntropyLoss()(dw, labels)"
      ],
      "metadata": {
        "id": "NGgrxXLR_ja5",
        "outputId": "72fb629b-7c66-4b1c-c7ac-8994739e9f94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.5059), tensor(0.5059))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 1\n",
        "\n",
        "**不**能直接使用`F.cross_entropy`或`nn.CrossEntropyLoss`"
      ],
      "metadata": {
        "id": "WkL6knQjFavy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_sum = []\n",
        "for letter, label in zip(dw, labels):\n",
        "  fs = F.softmax(letter, dim=1)\n",
        "  ylogyh = label*torch.log(fs)\n",
        "  print(ylogyh)\n",
        "  sumlog = torch.sum(ylogyh, dim=0)\n",
        "  ce_sum.append(sumlog)\n",
        "torch.stack(ce_sum), -torch.mean(torch.stack(ce_sum))"
      ],
      "metadata": {
        "id": "V9J7uLyEFaHj",
        "outputId": "85102766-d661-4f08-8341-f225115d9cd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0000, -1.8783, -0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000, -1.9616, -0.0000]])\n",
            "tensor([[-1.6103, -0.0000, -0.0000, -0.0000],\n",
            "        [-0.0000, -0.0000, -0.0000, -1.7867]])\n",
            "tensor([[-0.0000, -0.0000, -0.6721, -0.0000],\n",
            "        [-0.0000, -1.5270, -0.0000, -0.0000]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000, -1.8783, -1.9616,  0.0000],\n",
              "         [-1.6103,  0.0000,  0.0000, -1.7867],\n",
              "         [ 0.0000, -1.5270, -0.6721,  0.0000]]),\n",
              " tensor(0.7863))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CASE 2\n",
        "\n",
        "能直接使用`F.cross_entropy`或`nn.CrossEntropyLoss`"
      ],
      "metadata": {
        "id": "HmKuI-t6JO1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce = F.cross_entropy(dw, labels)\n",
        "cen = F.cross_entropy(dw, labels, reduction='none')\n",
        "cen, ce"
      ],
      "metadata": {
        "id": "QOuQ4j_BJSmS",
        "outputId": "92f0542a-b6f1-45dc-b108-fb3fa6984f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0000, 1.9912, 0.7264, -0.0000],\n",
              "         [0.5537, -0.0000, -0.0000, 0.8997],\n",
              "         [-0.0000, 1.8440, 0.0564, -0.0000]]),\n",
              " tensor(0.5059))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "手動計算 Cross Entropy"
      ],
      "metadata": {
        "id": "HKgSxbEHKe3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fsoft = F.softmax(dw, dim=1)\n",
        "fsoft"
      ],
      "metadata": {
        "id": "ud4liPePJuQC",
        "outputId": "fcaef64d-bae3-48a3-b560-e0465e822193",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1219, 0.1365, 0.5164, 0.8431],\n",
              "         [0.8781, 0.8635, 0.4836, 0.1569]],\n",
              "\n",
              "        [[0.5748, 0.2627, 0.8302, 0.5933],\n",
              "         [0.4252, 0.7373, 0.1698, 0.4067]],\n",
              "\n",
              "        [[0.5778, 0.8418, 0.9451, 0.9145],\n",
              "         [0.4222, 0.1582, 0.0549, 0.0855]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ylogyh = labels * torch.log(fsoft)\n",
        "sumlog = torch.sum(ylogyh, dim=1)\n",
        "cem = -torch.mean(sumlog)\n",
        "sumlog, cem"
      ],
      "metadata": {
        "id": "IzW__KT7LEmA",
        "outputId": "a652ef4d-bf42-4a29-fc42-50d8422b6fc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.0000, -1.9912, -0.7264,  0.0000],\n",
              "         [-0.5537,  0.0000,  0.0000, -0.8997],\n",
              "         [ 0.0000, -1.8440, -0.0564,  0.0000]]),\n",
              " tensor(0.5059))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Poy1YTR1MRv6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}