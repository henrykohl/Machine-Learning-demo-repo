{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrykohl/Machine-Learning-demo-repo/blob/master/jovian/deep-learning-with-pytorch/06b-anime-dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-24T06:12:29.332294Z",
          "iopub.status.busy": "2023-02-24T06:12:29.331719Z",
          "iopub.status.idle": "2023-02-24T06:12:29.346475Z",
          "shell.execute_reply": "2023-02-24T06:12:29.344686Z",
          "shell.execute_reply.started": "2023-02-24T06:12:29.332244Z"
        },
        "id": "u900af4g9JiD"
      },
      "source": [
        "# <font color=\"red\">(Lecture 6) Training Generative Adversarial Networks (GANs) in PyTorch</font>\n",
        "\n",
        "### <font color=\"#FF00FF\">*Part 7 of \"Deep Learning with Pytorch: Zero to GANs\"*</font>\n",
        "\n",
        "This tutorial series is a hands-on beginner-friendly introduction to deep learning using [PyTorch](https://pytorch.org), an open-source neural networks library. These tutorials take a practical and coding-focused approach. The best way to learn the material is to execute the code and experiment with it yourself. Check out the full series here:\n",
        "\n",
        "1. [PyTorch Basics: Tensors & Gradients](https://jovian.ai/aakashns/01-pytorch-basics)\n",
        "2. [Gradient Descent & Linear Regression](https://jovian.ai/aakashns/02-linear-regression)\n",
        "3. [Working with Images & Logistic Regression](https://jovian.ai/aakashns/03-logistic-regression)\n",
        "4. [Training Deep Neural Networks on a GPU](https://jovian.ai/aakashns/04-feedforward-nn)\n",
        "5. [Image Classification using Convolutional Neural Networks](https://jovian.ai/aakashns/05-cifar10-cnn)\n",
        "6. [Data Augmentation, Regularization and ResNets](https://jovian.ai/aakashns/05b-cifar10-resnet)\n",
        "7. [Generating Images using Generative Adversarial Networks](https://jovian.ai/aakashns/06b-anime-dcgan/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnpm6BIo9jUy"
      },
      "source": [
        "### How to run the code\n",
        "\n",
        "This tutorial is an executable [Jupyter notebook](https://jupyter.org) hosted on [Jovian](https://www.jovian.ai). You can _run_ this tutorial and experiment with the code examples in a couple of ways: *using free online resources* (recommended) or *on your computer*.\n",
        "\n",
        "#### Option 1: Running using free online resources (1-click, recommended)\n",
        "\n",
        "The easiest way to start executing the code is to click the **Run** button at the top of this page and select **Run on Colab**. [Google Colab](https://colab.research.google.com) is a free online platform for running Jupyter notebooks using Google's cloud infrastructure. You can also select \"Run on Binder\" or \"Run on Kaggle\" if you face issues running the notebook on Google Colab.\n",
        "\n",
        "\n",
        "#### Option 2: Running on your computer locally\n",
        "\n",
        "To run the code on your computer locally, you'll need to set up [Python](https://www.python.org), download the notebook and install the required libraries. We recommend using the [Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) distribution of Python. Click the **Run** button at the top of this page, select the **Run Locally** option, and follow the instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxlcwfri96Ec"
      },
      "source": [
        "### Using a GPU for faster training\n",
        "\n",
        "You can use a [Graphics Processing Unit](https://en.wikipedia.org/wiki/Graphics_processing_unit) (GPU) to train your models faster if your execution platform is connected to a GPU manufactured by NVIDIA. Follow these instructions to use a GPU on the platform of your choice:\n",
        "\n",
        "* _Google Colab_: Use the menu option \"Runtime > Change Runtime Type\" and select \"GPU\" from the \"Hardware Accelerator\" dropdown.\n",
        "* _Kaggle_: In the \"Settings\" section of the sidebar, select \"GPU\" from the \"Accelerator\" dropdown. Use the button on the top-right to open the sidebar.\n",
        "* _Binder_: Notebooks running on Binder cannot use a GPU, as the machines powering Binder aren't connected to any GPUs.\n",
        "* _Linux_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html).\n",
        "* _Windows_: If your laptop/desktop has an NVIDIA GPU (graphics card), make sure you have installed the [NVIDIA CUDA drivers](https://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/index.html).\n",
        "* _macOS_: macOS is not compatible with NVIDIA GPUs\n",
        "\n",
        "\n",
        "If you do not have access to a GPU or aren't sure what it is, don't worry, you can execute all the code in this tutorial just fine without a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h02mLy2t7rPM"
      },
      "source": [
        "# Introduction to Generative Modeling\n",
        "\n",
        "Deep neural networks are used mainly for supervised learning: classification or regression. Generative Adversarial Networks or GANs, however, use neural networks for a very different purpose: Generative modeling\n",
        "\n",
        "> Generative modeling is an unsupervised learning task in machine learning that involves automatically discovering and learning the regularities or patterns in input data in such a way that the model can be used to generate or output new examples that plausibly could have been drawn from the original dataset. - [Source](https://machinelearningmastery.com/what-are-generative-adversarial-networks-gans/)\n",
        "\n",
        "To get a sense of the power of generative models, just visit [thispersondoesnotexist.com](https://thispersondoesnotexist.com). Every time you reload the page, a new image of a person's face is generated on the fly. The results are pretty fascinating:\n",
        "\n",
        "<img src=\"https://imgix.bustle.com/inverse/4b/17/8f/0e/cf91/4506/99c7/e6a491c5d4ac/these-people-are-not-real--they-were-produced-by-our-generator-that-allows-control-over-different-a.png\" style=\"width:480px; margin-bottom:32px\"/>\n",
        "\n",
        "While there are many approaches used for generative modeling, a Generative Adversarial Network takes the following approach:\n",
        "\n",
        "<img src=\"https://i.imgur.com/6NMdO9u.png\" style=\"width:420px; margin-bottom:32px\"/>\n",
        "\n",
        "There are two neural networks: a *Generator* and a *Discriminator*. The generator generates a \"fake\" sample given a random vector/matrix, and the discriminator attempts to detect whether a given sample is \"real\" (picked from the training data) or \"fake\" (generated by the generator). Training happens in tandem: we train the discriminator for a few epochs, then train the generator for a few epochs, and repeat. This way both the generator and the discriminator get better at doing their jobs.\n",
        "\n",
        "GANs however, can be notoriously difficult to train, and are extremely sensitive to hyperparameters, activation functions and regularization. In this tutorial, we'll train a GAN to generate images of anime characters' faces.\n",
        "\n",
        "<img src=\"https://i.imgur.com/NaKtJs0.png\" width=\"360\" style=\"margin-bottom:32px\"/>\n",
        "\n",
        "\n",
        "We'll use the [Anime Face Dataset](https://github.com/Mckinsey666/Anime-Face-Dataset), which consists of over 63,000 cropped anime faces. Note that generative modeling is an unsupervised learning task, so the images do not have any labels. Most of the code in this tutorial is based [on this notebook](https://www.kaggle.com/splcher/starter-anime-face-dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:51:16.493127Z",
          "iopub.status.busy": "2023-06-13T10:51:16.492242Z",
          "iopub.status.idle": "2023-06-13T10:51:25.338087Z",
          "shell.execute_reply": "2023-06-13T10:51:25.336897Z",
          "shell.execute_reply.started": "2023-06-13T10:51:16.493021Z"
        },
        "id": "KRqKEkyXS7IL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:52:13.046568Z",
          "iopub.status.busy": "2023-06-13T10:52:13.046111Z",
          "iopub.status.idle": "2023-06-13T10:52:13.116718Z",
          "shell.execute_reply": "2023-06-13T10:52:13.115614Z",
          "shell.execute_reply.started": "2023-06-13T10:52:13.046529Z"
        },
        "id": "4z-CnqGhS7IS",
        "outputId": "b66dcaca-ea2f-47ce-d89e-258c06c1e7ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.9921, 0.0049, 0.0031]], device='cuda:0')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp = torch.tensor([[0.9921, 0.0049, 0.0031]])\n",
        "temp.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:52:32.871495Z",
          "iopub.status.busy": "2023-06-13T10:52:32.871064Z",
          "iopub.status.idle": "2023-06-13T10:52:32.880047Z",
          "shell.execute_reply": "2023-06-13T10:52:32.878888Z",
          "shell.execute_reply.started": "2023-06-13T10:52:32.871458Z"
        },
        "id": "RgMOsO5RS7IY",
        "outputId": "9792ebeb-c036-49cb-cab1-69b2f44d7dc1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "temp.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:52:53.115735Z",
          "iopub.status.busy": "2023-06-13T10:52:53.114759Z",
          "iopub.status.idle": "2023-06-13T10:52:53.124443Z",
          "shell.execute_reply": "2023-06-13T10:52:53.123372Z",
          "shell.execute_reply.started": "2023-06-13T10:52:53.115693Z"
        },
        "id": "17w9I5w4S7Ic"
      },
      "outputs": [],
      "source": [
        "model2 = torch.nn.Sequential(\n",
        "  torch.nn.Linear(3,1)         # 輸入3個值, 輸出1個值\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:53:21.861945Z",
          "iopub.status.busy": "2023-06-13T10:53:21.861512Z",
          "iopub.status.idle": "2023-06-13T10:53:21.872877Z",
          "shell.execute_reply": "2023-06-13T10:53:21.871532Z",
          "shell.execute_reply.started": "2023-06-13T10:53:21.861888Z"
        },
        "id": "RrwvMcyhS7Ig",
        "outputId": "f71401ca-4a5c-452c-d19a-113169d17bb7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=3, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model2.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:57:23.658906Z",
          "iopub.status.busy": "2023-06-13T10:57:23.658444Z",
          "iopub.status.idle": "2023-06-13T10:57:23.666016Z",
          "shell.execute_reply": "2023-06-13T10:57:23.664707Z",
          "shell.execute_reply.started": "2023-06-13T10:57:23.658866Z"
        },
        "id": "KrhWk4lXS7Ii",
        "outputId": "985a0943-84b3-4c95-e05e-a9f3d22d1466"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.nn.modules.container.Sequential"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:54:01.857097Z",
          "iopub.status.busy": "2023-06-13T10:54:01.856681Z",
          "iopub.status.idle": "2023-06-13T10:54:01.866055Z",
          "shell.execute_reply": "2023-06-13T10:54:01.865044Z",
          "shell.execute_reply.started": "2023-06-13T10:54:01.857060Z"
        },
        "id": "crLYCATpS7Il",
        "outputId": "c1567837-f709-4e73-a270-4a18c0f67c7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[0.2406, 0.3178, 0.1263]], device='cuda:0', requires_grad=True)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "next(iter(model2.parameters()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-01T10:31:54.698289Z",
          "iopub.status.busy": "2023-03-01T10:31:54.697829Z",
          "iopub.status.idle": "2023-03-01T10:31:54.704429Z",
          "shell.execute_reply": "2023-03-01T10:31:54.703184Z",
          "shell.execute_reply.started": "2023-03-01T10:31:54.698251Z"
        },
        "id": "LmunuuEa7rPN"
      },
      "outputs": [],
      "source": [
        "project_name = '06b-anime-dcgan-live'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:56:23.401604Z",
          "iopub.status.busy": "2023-06-13T10:56:23.400869Z",
          "iopub.status.idle": "2023-06-13T10:56:23.430585Z",
          "shell.execute_reply": "2023-06-13T10:56:23.429236Z",
          "shell.execute_reply.started": "2023-06-13T10:56:23.401565Z"
        },
        "id": "RJup4264S7Is",
        "outputId": "46c953a5-d081-430a-de23-fff53cf8b842"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_24/1018717875.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
          ]
        }
      ],
      "source": [
        "output = model2(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-06-13T10:56:04.534533Z",
          "iopub.status.busy": "2023-06-13T10:56:04.534071Z",
          "iopub.status.idle": "2023-06-13T10:56:04.544119Z",
          "shell.execute_reply": "2023-06-13T10:56:04.542944Z",
          "shell.execute_reply.started": "2023-06-13T10:56:04.534492Z"
        },
        "id": "P_It4vYsS7Iv",
        "outputId": "b03aa36f-1631-4609-8709-1c2150fb4a33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[0.7856]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zESxd3c7rPN"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run the appropriate command for your operating system, if required\n",
        "# No installation is reqiured on Google Colab / Kaggle notebooks\n",
        "\n",
        "# Linux / Binder / Windows (No GPU)\n",
        "# !pip install numpy matplotlib torch==1.7.0+cpu torchvision==0.8.1+cpu torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# Linux / Windows (GPU)\n",
        "# pip install numpy matplotlib torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# MacOS (NO GPU)\n",
        "# !pip install numpy matplotlib torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-08CHC57rPO"
      },
      "source": [
        "## Downloading and Exploring the Data\n",
        "\n",
        "We can use the [`opendatasets`](https://github.com/JovianML/opendatasets) library to download the [dataset](https://www.kaggle.com/splcher/animefacedataset) from Kaggle. `opendatasets` uses the [Kaggle Official API](https://github.com/Kaggle/kaggle-api) for downloading datasets from Kaggle.  Follow these steps to find your API credentials:\n",
        "\n",
        "1. Sign in to  [https://kaggle.com/](https://kaggle.com),  then click on your profile picture on the top right and select \"My Account\" from the menu.\n",
        "\n",
        "2. Scroll down to the \"API\" section and click \"Create New API Token\". This will download a file `kaggle.json` with the following contents:\n",
        "\n",
        "```\n",
        "{\"username\":\"YOUR_KAGGLE_USERNAME\",\"key\":\"YOUR_KAGGLE_KEY\"}\n",
        "```\n",
        "\n",
        "3. When you run `opendatsets.download`, you will be asked to enter your username & Kaggle API, which you can get from the file downloaded in step 2.\n",
        "\n",
        "Note that you need to download the `kaggle.json` file only once. On Google Colab, you can also upload the `kaggle.json` file using the files tab, and the credentials will be read automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-07T04:17:47.310754Z",
          "iopub.status.busy": "2023-03-07T04:17:47.310196Z",
          "iopub.status.idle": "2023-03-07T04:18:01.284022Z",
          "shell.execute_reply": "2023-03-07T04:18:01.282815Z",
          "shell.execute_reply.started": "2023-03-07T04:17:47.310646Z"
        },
        "id": "Drxb-KX6BKPw"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets --upgrade --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGgWY_lUS7I2"
      },
      "source": [
        "<font color=\"pink\">username:</font> henryhsu  \n",
        "<font color=\"pink\">key:</font> 34e4312e74cc9b9a6c726a3c834c2af9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T06:59:01.319694Z",
          "iopub.status.busy": "2023-03-06T06:59:01.319224Z",
          "iopub.status.idle": "2023-03-06T06:59:42.338623Z",
          "shell.execute_reply": "2023-03-06T06:59:42.337410Z",
          "shell.execute_reply.started": "2023-03-06T06:59:01.319652Z"
        },
        "id": "1ffs2UlyBQhi"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/splcher/animefacedataset'\n",
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiMLxHRL7rPO"
      },
      "source": [
        "The dataset has a single folder called `images` which contains all 63,000+ images in JPG format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:00:23.596344Z",
          "iopub.status.busy": "2023-03-06T07:00:23.594877Z",
          "iopub.status.idle": "2023-03-06T07:00:23.602820Z",
          "shell.execute_reply": "2023-03-06T07:00:23.601770Z",
          "shell.execute_reply.started": "2023-03-06T07:00:23.596285Z"
        },
        "id": "p2VLPRoq7rPP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = './animefacedataset'\n",
        "print(os.listdir(DATA_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:00:27.839138Z",
          "iopub.status.busy": "2023-03-06T07:00:27.838543Z",
          "iopub.status.idle": "2023-03-06T07:00:27.884142Z",
          "shell.execute_reply": "2023-03-06T07:00:27.882828Z",
          "shell.execute_reply.started": "2023-03-06T07:00:27.839063Z"
        },
        "id": "hg9EwokF7rPQ"
      },
      "outputs": [],
      "source": [
        "print(os.listdir(DATA_DIR+'/images')[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDRRG5907rPQ"
      },
      "source": [
        "Let's load this dataset using the `ImageFolder` class from `torchvision`. We will also resize and crop the images to 64x64 px, and normalize the pixel values with a mean & standard deviation of 0.5 for each channel. This will ensure that pixel values are in the range `(-1, 1)`, which is more  convenient for training the discriminator. We will also create a data loader to load the data in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:00:35.412754Z",
          "iopub.status.busy": "2023-03-06T07:00:35.412039Z",
          "iopub.status.idle": "2023-03-06T07:00:37.631595Z",
          "shell.execute_reply": "2023-03-06T07:00:37.630149Z",
          "shell.execute_reply.started": "2023-03-06T07:00:35.412685Z"
        },
        "id": "hmRajdDe7rPR"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:00:43.685204Z",
          "iopub.status.busy": "2023-03-06T07:00:43.684597Z",
          "iopub.status.idle": "2023-03-06T07:00:43.691114Z",
          "shell.execute_reply": "2023-03-06T07:00:43.689490Z",
          "shell.execute_reply.started": "2023-03-06T07:00:43.685166Z"
        },
        "id": "OSlbIxS37rPR"
      },
      "outputs": [],
      "source": [
        "image_size = 64\n",
        "batch_size = 128\n",
        "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:00:49.802127Z",
          "iopub.status.busy": "2023-03-06T07:00:49.801540Z",
          "iopub.status.idle": "2023-03-06T07:00:50.112588Z",
          "shell.execute_reply": "2023-03-06T07:00:50.111607Z",
          "shell.execute_reply.started": "2023-03-06T07:00:49.802047Z"
        },
        "id": "aLp6NN8B7rPR"
      },
      "outputs": [],
      "source": [
        "train_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n",
        "    T.Resize(image_size),\n",
        "    T.CenterCrop(image_size),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(*stats)]))\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSFOUt4XS7JE"
      },
      "source": [
        "<font color=\"red\">注意：T.Normalize是layer normalization</font> <br>\n",
        "https://blog.csdn.net/qq_36575363/article/details/110292221"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n50e7mfZ7rPS"
      },
      "source": [
        "Let's create helper functions to denormalize the image tensors and display some sample images from a training batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:48:56.337785Z",
          "iopub.status.busy": "2023-03-06T07:48:56.337307Z",
          "iopub.status.idle": "2023-03-06T07:48:56.344268Z",
          "shell.execute_reply": "2023-03-06T07:48:56.342995Z",
          "shell.execute_reply.started": "2023-03-06T07:48:56.337750Z"
        },
        "id": "tPVrSXO17rPS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline #如果不加这一句的话，我们在画图结束之后需要加上plt.show()才可以显示图像。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:25:15.280364Z",
          "iopub.status.busy": "2023-03-06T07:25:15.279892Z",
          "iopub.status.idle": "2023-03-06T07:25:15.286574Z",
          "shell.execute_reply": "2023-03-06T07:25:15.285321Z",
          "shell.execute_reply.started": "2023-03-06T07:25:15.280326Z"
        },
        "id": "lJTY6BMf7rPS"
      },
      "outputs": [],
      "source": [
        "def denorm(img_tensors):\n",
        "    return img_tensors * stats[1][0] + stats[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:49:02.703338Z",
          "iopub.status.busy": "2023-03-06T07:49:02.702610Z",
          "iopub.status.idle": "2023-03-06T07:49:02.709820Z",
          "shell.execute_reply": "2023-03-06T07:49:02.708957Z",
          "shell.execute_reply.started": "2023-03-06T07:49:02.703289Z"
        },
        "id": "Jntkukw47rPT"
      },
      "outputs": [],
      "source": [
        "def show_images(images, nmax=64):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "#     fig, ax = plt.subplots(figsize=(16, 16)) # plt.subplots(2, 3) 即表示一次性在figure上創建成2*3的網格\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid(denorm(images.detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
        "#     plt.show() # 這裡不需要\n",
        "\n",
        "def show_batch(dl, nmax=64):\n",
        "    for images, lbs in dl:\n",
        "        print(lbs.shape)\n",
        "        show_images(images, nmax)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-30T06:07:19.770384Z",
          "iopub.status.busy": "2023-01-30T06:07:19.769797Z",
          "iopub.status.idle": "2023-01-30T06:07:19.778128Z",
          "shell.execute_reply": "2023-01-30T06:07:19.776920Z",
          "shell.execute_reply.started": "2023-01-30T06:07:19.770351Z"
        },
        "id": "45OBjY-oS7JJ"
      },
      "outputs": [],
      "source": [
        "# from part5 (測試)\n",
        "# stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# def denormalize(images, means, stds):\n",
        "#     means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
        "#     stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
        "#     print(means.shape, stds.shape)\n",
        "#     return images * stds + means\n",
        "\n",
        "# def show_batch_2(dl):\n",
        "#     for images, labels in dl:\n",
        "#         print(labels.shape)\n",
        "#         fig, ax = plt.subplots(figsize=(8, 8))\n",
        "#         ax.set_xticks([]); ax.set_yticks([])\n",
        "#         denorm_images = denormalize(images, *stats)\n",
        "#         ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0))\n",
        "#         break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T06:41:47.680826Z",
          "iopub.status.busy": "2023-02-01T06:41:47.680317Z",
          "iopub.status.idle": "2023-02-01T06:41:48.533829Z",
          "shell.execute_reply": "2023-02-01T06:41:48.532618Z",
          "shell.execute_reply.started": "2023-02-01T06:41:47.680769Z"
        },
        "id": "laQtLhl_7rPT"
      },
      "outputs": [],
      "source": [
        "# show_batch(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-21T12:12:55.448244Z",
          "iopub.status.busy": "2023-01-21T12:12:55.447891Z",
          "iopub.status.idle": "2023-01-21T12:12:56.020848Z",
          "shell.execute_reply": "2023-01-21T12:12:56.019438Z",
          "shell.execute_reply.started": "2023-01-21T12:12:55.448215Z"
        },
        "id": "LYI_ULHBS7JL"
      },
      "outputs": [],
      "source": [
        "# show_batch_2(train_dl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-21T12:04:23.563046Z",
          "iopub.status.busy": "2023-01-21T12:04:23.562699Z",
          "iopub.status.idle": "2023-01-21T12:04:23.570093Z",
          "shell.execute_reply": "2023-01-21T12:04:23.568983Z",
          "shell.execute_reply.started": "2023-01-21T12:04:23.563018Z"
        },
        "id": "Z9Sa-D4_S7JM"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# sst1 = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# sst2 = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "# def fintsst(a1, b1):\n",
        "#     print(a1,\" | \", b1)\n",
        "\n",
        "# fintsst(*sst1)\n",
        "# fintsst(*sst2)\n",
        "# 結果相同"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:49:10.129030Z",
          "iopub.status.busy": "2023-03-06T07:49:10.128237Z",
          "iopub.status.idle": "2023-03-06T07:49:22.003345Z",
          "shell.execute_reply": "2023-03-06T07:49:22.001371Z",
          "shell.execute_reply.started": "2023-03-06T07:49:10.128987Z"
        },
        "id": "21mqBm7w7rPU"
      },
      "outputs": [],
      "source": [
        "!pip install jovian --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T07:49:22.007950Z",
          "iopub.status.busy": "2023-03-06T07:49:22.007347Z",
          "iopub.status.idle": "2023-03-06T07:49:22.013457Z",
          "shell.execute_reply": "2023-03-06T07:49:22.012508Z",
          "shell.execute_reply.started": "2023-03-06T07:49:22.007894Z"
        },
        "id": "xrVpwse67rPV"
      },
      "outputs": [],
      "source": [
        "import jovian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCz65LzX7rPV"
      },
      "outputs": [],
      "source": [
        "jovian.commit(project=project_name, environment=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJWM0w8-7rPW"
      },
      "source": [
        "## Using a GPU\n",
        "\n",
        "To seamlessly use a GPU, if one is available, we define a couple of helper functions (`get_default_device` & `to_device`) and a helper class `DeviceDataLoader` to move our model & data to the GPU, if one is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:09:30.288042Z",
          "iopub.status.busy": "2023-03-06T08:09:30.287614Z",
          "iopub.status.idle": "2023-03-06T08:09:30.298234Z",
          "shell.execute_reply": "2023-03-06T08:09:30.296677Z",
          "shell.execute_reply.started": "2023-03-06T08:09:30.288009Z"
        },
        "id": "fb8ORjt57rPX"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRxpHAp67rPX"
      },
      "source": [
        "Based on where you're running this notebook, your default device could be a CPU (`torch.device('cpu')`) or a GPU (`torch.device('cuda')`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:09:39.026599Z",
          "iopub.status.busy": "2023-03-06T08:09:39.025763Z",
          "iopub.status.idle": "2023-03-06T08:09:39.036708Z",
          "shell.execute_reply": "2023-03-06T08:09:39.035050Z",
          "shell.execute_reply.started": "2023-03-06T08:09:39.026547Z"
        },
        "id": "yutNeU5w7rPX"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TOp0Zwt7rPY"
      },
      "source": [
        "We can now move our training data loader using `DeviceDataLoader` for automatically transferring batches of data to the GPU (if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:09:44.794869Z",
          "iopub.status.busy": "2023-03-06T08:09:44.793864Z",
          "iopub.status.idle": "2023-03-06T08:09:44.800007Z",
          "shell.execute_reply": "2023-03-06T08:09:44.798781Z",
          "shell.execute_reply.started": "2023-03-06T08:09:44.794826Z"
        },
        "id": "ryTEXQuD7rPY"
      },
      "outputs": [],
      "source": [
        "train_dl_g = DeviceDataLoader(train_dl, device)\n",
        "# train_dl_g 是一個迭代器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoOJIJ937rPY"
      },
      "source": [
        "## Discriminator Network\n",
        "\n",
        "The discriminator takes an image as input, and tries to classify it as \"real\" or \"generated\". In this sense, it's like any other neural network. We'll use a convolutional neural networks (CNN) which outputs a single number output for every image. We'll use stride of 2 to progressively reduce the size of the output feature map.\n",
        "\n",
        "![](https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:13:37.514769Z",
          "iopub.status.busy": "2023-03-06T08:13:37.514344Z",
          "iopub.status.idle": "2023-03-06T08:13:37.520066Z",
          "shell.execute_reply": "2023-03-06T08:13:37.518873Z",
          "shell.execute_reply.started": "2023-03-06T08:13:37.514733Z"
        },
        "id": "htcCHpje7rPZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:14:08.146837Z",
          "iopub.status.busy": "2023-03-06T08:14:08.146386Z",
          "iopub.status.idle": "2023-03-06T08:14:08.194368Z",
          "shell.execute_reply": "2023-03-06T08:14:08.193184Z",
          "shell.execute_reply.started": "2023-03-06T08:14:08.146780Z"
        },
        "id": "6nq2pxPJ7rPZ"
      },
      "outputs": [],
      "source": [
        "discriminator = nn.Sequential(\n",
        "    # in: 3 x 64 x 64\n",
        "\n",
        "    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 64 x 32 x 32 因為 (64+2-4)/2 + 1 = 32\n",
        "\n",
        "    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 128 x 16 x 16 因為 (32+2-4)/2 + 1 = 16\n",
        "\n",
        "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 256 x 8 x 8 因為 (16+2-4)/2 + 1 = 8\n",
        "\n",
        "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 512 x 4 x 4 因為 (8+2-4)/2 + 1 = 4\n",
        "\n",
        "    nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    # out: 1 x 1 x 1 因為 4/4 = 1  因為 (4+0-4)/2 + 1 = 1\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Sigmoid())\n",
        "# 輸出值 介於 0 與 1 之間"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_R3kimmS7K9"
      },
      "source": [
        "#### <font color=\"red\">torch.flatten()與torch.nn.Flatten()</font> <br> https://blog.csdn.net/Super_user_and_woner/article/details/120782656"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzjyaUaC7rPZ"
      },
      "source": [
        "Note that we're using the Leaky ReLU activation for the discriminator.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ypsvQH7kvtI2BhzR2eT_Sw.png\" width=\"420\">\n",
        "\n",
        "\n",
        ">  Different from the regular ReLU function, Leaky ReLU allows the pass of a small gradient signal for negative values. As a result, it makes the gradients from the discriminator flows stronger into the generator. Instead of passing a gradient (slope) of 0 in the back-prop pass, it passes a small negative gradient.  - [Source](https://sthalles.github.io/advanced_gans/)\n",
        "\n",
        "Just like any other binary classification model, the output of the discriminator is a single number between 0 and 1, which can be interpreted as the probability of the input image being real i.e. picked from the original dataset.\n",
        "\n",
        "Let's move the discriminator model to the chosen device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:14:16.561803Z",
          "iopub.status.busy": "2023-03-06T08:14:16.561388Z",
          "iopub.status.idle": "2023-03-06T08:14:16.571518Z",
          "shell.execute_reply": "2023-03-06T08:14:16.570267Z",
          "shell.execute_reply.started": "2023-03-06T08:14:16.561769Z"
        },
        "id": "tFT_FGnN7rPZ"
      },
      "outputs": [],
      "source": [
        "discriminator = to_device(discriminator, device)\n",
        "discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HdpVR1n7rPa"
      },
      "source": [
        "## Generator Network\n",
        "\n",
        "The input to the generator is typically a vector or a matrix of random numbers (referred to as a latent tensor) which is used as a seed for generating an image. The generator will convert a latent tensor of shape `(128, 1, 1)` into an image tensor of shape `3 x 64 x 64`. To achive this, we'll use the `ConvTranspose2d` layer from PyTorch, which is performs to as a *transposed convolution* (also referred to as a *deconvolution*). [Learn more](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md#transposed-convolution-animations)\n",
        "\n",
        "![](https://i.imgur.com/DRvK546.gif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:14:23.710623Z",
          "iopub.status.busy": "2023-03-06T08:14:23.710122Z",
          "iopub.status.idle": "2023-03-06T08:14:23.717447Z",
          "shell.execute_reply": "2023-03-06T08:14:23.716098Z",
          "shell.execute_reply.started": "2023-03-06T08:14:23.710586Z"
        },
        "id": "496i2VzW7rPa"
      },
      "outputs": [],
      "source": [
        "latent_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:14:55.918560Z",
          "iopub.status.busy": "2023-03-06T08:14:55.917989Z",
          "iopub.status.idle": "2023-03-06T08:14:55.970225Z",
          "shell.execute_reply": "2023-03-06T08:14:55.969198Z",
          "shell.execute_reply.started": "2023-03-06T08:14:55.918514Z"
        },
        "id": "XuDQRN827rPc"
      },
      "outputs": [],
      "source": [
        "generator = nn.Sequential(\n",
        "    # in: latent_size x 1 x 1\n",
        "    # s(i'-1)+k-2p=o'\n",
        "    nn.ConvTranspose2d(latent_size, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(True),\n",
        "    # out: 512 x 4 x 4     # 1*(1-1)+4-2*0 = 4\n",
        "\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(True),\n",
        "    # out: 256 x 8 x 8    # 2*(4-1)+4-2*1 = 8\n",
        "\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(True),\n",
        "    # out: 128 x 16 x 16  # 2*(8-1)+4-2*1 = 16\n",
        "\n",
        "    nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(64),\n",
        "    nn.ReLU(True),\n",
        "    # out: 64 x 32 x 32  # 2*(16-1)+4-2*1 = 32\n",
        "\n",
        "    nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.Tanh()\n",
        "    # out: 3 x 64 x 64   # 2*(32-1)+4-2*1 = 64\n",
        ")\n",
        "\n",
        "# 輸出值 介於 -1 與 1 之間"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T4IiCB97rPc"
      },
      "source": [
        "We use the TanH activation function for the output layer of the generator.\n",
        "\n",
        "<img src=\"https://nic.schraudolph.org/teach/NNcourse/figs/tanh.gif\" width=\"420\" >\n",
        "\n",
        "> \"The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output layer which uses the Tanh function. We observed that using a bounded activation allowed the model to learn more quickly to saturate and cover the color space of the training distribution. Within the discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work well, especially for higher resolution modeling.\" - [Source](https://stackoverflow.com/questions/41489907/generative-adversarial-networks-tanh)\n",
        "\n",
        "\n",
        "Note that since the outputs of the TanH activation lie in the range `[-1,1]`, we have applied the similar transformation to the images in the training dataset. Let's generate some outputs using the generator and view them as images by transforming and denormalizing the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:18:34.180715Z",
          "iopub.status.busy": "2023-03-06T08:18:34.180258Z",
          "iopub.status.idle": "2023-03-06T08:18:35.353318Z",
          "shell.execute_reply": "2023-03-06T08:18:35.352368Z",
          "shell.execute_reply.started": "2023-03-06T08:18:34.180679Z"
        },
        "id": "9_aHHWA07rPd"
      },
      "outputs": [],
      "source": [
        "xb = torch.randn(batch_size, latent_size, 1, 1) # random latent tensors\n",
        "fake_images = generator(xb)  # fake_images中每一個值 介於-1與1之間\n",
        "print(fake_images.shape)     # [128, 3, 64, 64]\n",
        "show_images(fake_images)     # show_images函數中有 包含 denorm ， 所以值 會變成介於0與1之間"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-29T13:54:34.116898Z",
          "iopub.status.busy": "2023-01-29T13:54:34.115710Z",
          "iopub.status.idle": "2023-01-29T13:54:34.123840Z",
          "shell.execute_reply": "2023-01-29T13:54:34.122743Z",
          "shell.execute_reply.started": "2023-01-29T13:54:34.116846Z"
        },
        "id": "V5A12Q4cS7LS"
      },
      "outputs": [],
      "source": [
        "# 測試 原輸入\n",
        "# xb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-30T12:59:38.189330Z",
          "iopub.status.busy": "2023-01-30T12:59:38.188911Z",
          "iopub.status.idle": "2023-01-30T12:59:38.195104Z",
          "shell.execute_reply": "2023-01-30T12:59:38.193847Z",
          "shell.execute_reply.started": "2023-01-30T12:59:38.189297Z"
        },
        "id": "bqcYqGWJS7LT"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# xt = torch.randn(1, 128, 1, 1) # 值可能會>1，也有可能<-1\n",
        "# xt\n",
        "# f_images = generator(xt)       # 值介於1與-1之間\n",
        "# print(f_images.shape)\n",
        "# f_images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6CwMocE7rPd"
      },
      "source": [
        "As one might expect, the output from the generator is basically random noise, since we haven't trained it yet.\n",
        "\n",
        "Let's move the generator to the chosen device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T08:18:44.651733Z",
          "iopub.status.busy": "2023-03-06T08:18:44.650707Z",
          "iopub.status.idle": "2023-03-06T08:18:44.657100Z",
          "shell.execute_reply": "2023-03-06T08:18:44.655819Z",
          "shell.execute_reply.started": "2023-03-06T08:18:44.651694Z"
        },
        "id": "WhsPkkCP7rPe"
      },
      "outputs": [],
      "source": [
        "generator = to_device(generator, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHzgNaR57rPe"
      },
      "source": [
        "## Discriminator Training\n",
        "\n",
        "Since the discriminator is a binary classification model, we can use the binary cross entropy loss function to quantify how well it is able to differentiate between real and generated images.\n",
        "\n",
        "<img src=\"https://image.slidesharecdn.com/chrishokamp-dublinnlp3-160805110319/95/task-based-learning-for-nlp-going-beyond-cross-entropy-chris-hokamp-10-638.jpg?cb=1470395213\" width=\"420\" >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-30T06:38:12.677101Z",
          "iopub.status.busy": "2023-01-30T06:38:12.676578Z",
          "iopub.status.idle": "2023-01-30T06:38:12.686487Z",
          "shell.execute_reply": "2023-01-30T06:38:12.684960Z",
          "shell.execute_reply.started": "2023-01-30T06:38:12.677059Z"
        },
        "id": "RdGJ1juuS7LX"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# t = torch.ones(128, 1, device=device)\n",
        "# t.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T09:33:10.308311Z",
          "iopub.status.busy": "2023-03-06T09:33:10.307853Z",
          "iopub.status.idle": "2023-03-06T09:33:10.318873Z",
          "shell.execute_reply": "2023-03-06T09:33:10.317620Z",
          "shell.execute_reply.started": "2023-03-06T09:33:10.308274Z"
        },
        "id": "xz3jTbye7rPe"
      },
      "outputs": [],
      "source": [
        "#   輸入 real_images 128x3x64x64 值介於 -1 與 1 之間\n",
        "def train_discriminator(real_images, opt_d):                      # 真實圖檔, optimizer\n",
        "    # Clear discriminator gradients\n",
        "    opt_d.zero_grad()\n",
        "\n",
        "    # Pass real images through discriminator (已把下兩行順序對調方便比較real與fake)\n",
        "    real_targets = torch.ones(real_images.size(0), 1, device=device)  # 128x1 (值為1)\n",
        "    real_preds = discriminator(real_images)                           # 128x1x1x1 (0 < 值 < 1)\n",
        "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
        "    real_score = torch.mean(real_preds).item()\n",
        "\n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device) # 128x128x1x1 (值:正態分佈)\n",
        "    fake_images = generator(latent)                                    # 123x3x64x64 (-1<值<1)\n",
        "\n",
        "    # Pass fake images through discriminator\n",
        "    fake_targets = torch.zeros(fake_images.size(0), 1, device=device)  # 128x1 (值為0)\n",
        "    fake_preds = discriminator(fake_images)                            # 128x1x1x1 (0 < 值 < 1)\n",
        "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
        "    fake_score = torch.mean(fake_preds).item()\n",
        "\n",
        "    # Update discriminator weights\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    opt_d.step()\n",
        "    return loss.item(), real_score, fake_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T04:35:41.286237Z",
          "iopub.status.busy": "2023-02-01T04:35:41.285761Z",
          "iopub.status.idle": "2023-02-01T04:35:41.295203Z",
          "shell.execute_reply": "2023-02-01T04:35:41.293966Z",
          "shell.execute_reply.started": "2023-02-01T04:35:41.286197Z"
        },
        "id": "9V_yhxBVS7La"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# real_targets = torch.ones(128, 1, device=device)\n",
        "# real_targets.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T04:37:06.443956Z",
          "iopub.status.busy": "2023-02-01T04:37:06.443560Z",
          "iopub.status.idle": "2023-02-01T04:37:06.450959Z",
          "shell.execute_reply": "2023-02-01T04:37:06.449717Z",
          "shell.execute_reply.started": "2023-02-01T04:37:06.443924Z"
        },
        "id": "FysmlaJqS7Lb"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# fake_targets = torch.zeros(128, 1, device=device)\n",
        "# fake_targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKyS1SKA7rPe"
      },
      "source": [
        "Here are the steps involved in training the discriminator.\n",
        "\n",
        "- We expect the discriminator to output <font color=\"red\">1</font> if the image was picked from the <font color=\"red\">real MNIST dataset</font>, and <font color=\"pink\">0</font> if it was generated using the <font color=\"pink\">generator network</font>.\n",
        "\n",
        "- We first pass a batch of <font color=\"red\">real images</font>, and compute the <font color=\"yellow\">loss</font>, setting the target labels to <font color=\"red\">1</font>.\n",
        "\n",
        "- Then we pass a batch of <font color=\"pink\">fake images</font> (generated using the generator) pass them into the discriminator, and compute the <font color=\"yellow\">loss</font>, setting the target labels to <font color=\"pink\">0</font>.\n",
        "\n",
        "- Finally we add the <font color=\"yellow\">two losses</font> and use the <font color=\"yellow\">overall loss</font> to perform gradient descent to adjust the weights of the discriminator.\n",
        "\n",
        "It's important to note that we don't change the weights of the `generator` model while training the `discriminator` (`opt_d` only affects the `discriminator.parameters()`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt_tkA547rPf"
      },
      "source": [
        "## Generator Training\n",
        "\n",
        "Since the outputs of the generator are images, it's not obvious how we can train the generator. This is where we employ a rather elegant trick, which is to use the discriminator as a part of the loss function. Here's how it works:\n",
        "\n",
        "- We generate a batch of images using the generator, pass the into the discriminator.\n",
        "\n",
        "- <font color=\"red\">We calculate the loss by setting the target labels to 1 i.e. real. We do this because the generator's objective is to \"fool\" the discriminator.</font>\n",
        "\n",
        "- We use the loss to perform gradient descent i.e. change the weights of the generator, so it gets better at generating real-like images to \"fool\" the discriminator.\n",
        "\n",
        "Here's what this looks like in code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T09:33:24.038099Z",
          "iopub.status.busy": "2023-03-06T09:33:24.037421Z",
          "iopub.status.idle": "2023-03-06T09:33:24.045690Z",
          "shell.execute_reply": "2023-03-06T09:33:24.044453Z",
          "shell.execute_reply.started": "2023-03-06T09:33:24.038049Z"
        },
        "id": "5lmjNkZE7rPf"
      },
      "outputs": [],
      "source": [
        "# (由generator) 產生 fake_images 值介於 -1 與 1 之間\n",
        "def train_generator(opt_g):\n",
        "    # Clear generator gradients\n",
        "    opt_g.zero_grad()\n",
        "\n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1, device=device) # 128x128x1x1 ( 值正態分佈 )\n",
        "    fake_images = generator(latent)                                    # 128x3x64x64 ( -1< 值 <1 )\n",
        "\n",
        "    # Try to fool the discriminator (已把下兩行順序對調方便與discriminator的fake部分 比較)\n",
        "    targets = torch.ones(batch_size, 1, device=device)                 # 128x1      (  值 = 1  )\n",
        "    preds = discriminator(fake_images)                                 # 128x1x1x1  (  0< 值 <1  )\n",
        "    loss = F.binary_cross_entropy(preds, targets)\n",
        "    # (不需要) score = torch.mean(preds).item()\n",
        "\n",
        "\n",
        "    # Update generator weights\n",
        "    loss.backward()\n",
        "    opt_g.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T07:21:46.402332Z",
          "iopub.status.busy": "2023-01-31T07:21:46.401943Z",
          "iopub.status.idle": "2023-01-31T07:21:46.410594Z",
          "shell.execute_reply": "2023-01-31T07:21:46.409450Z",
          "shell.execute_reply.started": "2023-01-31T07:21:46.402301Z"
        },
        "id": "2kwA-QxMS7Li"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# targets = torch.ones(batch_size, 1, device=device)\n",
        "# targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo00Pcmu7rPg"
      },
      "source": [
        "Let's create a directory where we can save intermediate outputs from the generator to visually inspect the progress of the model. We'll also create a helper function to export the generated images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T09:40:15.830612Z",
          "iopub.status.busy": "2023-03-06T09:40:15.830157Z",
          "iopub.status.idle": "2023-03-06T09:40:15.836036Z",
          "shell.execute_reply": "2023-03-06T09:40:15.834581Z",
          "shell.execute_reply.started": "2023-03-06T09:40:15.830576Z"
        },
        "id": "kiQyhcI37rPg"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T09:55:59.514151Z",
          "iopub.status.busy": "2023-03-06T09:55:59.513707Z",
          "iopub.status.idle": "2023-03-06T09:55:59.520476Z",
          "shell.execute_reply": "2023-03-06T09:55:59.519202Z",
          "shell.execute_reply.started": "2023-03-06T09:55:59.514114Z"
        },
        "id": "OhMPV5bk7rPg"
      },
      "outputs": [],
      "source": [
        "sample_dir = 'generated'\n",
        "os.makedirs(sample_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T09:55:59.588504Z",
          "iopub.status.busy": "2023-03-06T09:55:59.587871Z",
          "iopub.status.idle": "2023-03-06T09:55:59.596413Z",
          "shell.execute_reply": "2023-03-06T09:55:59.595411Z",
          "shell.execute_reply.started": "2023-03-06T09:55:59.588458Z"
        },
        "id": "LiCEUuDE7rPg"
      },
      "outputs": [],
      "source": [
        "def save_samples(index, latent_tensors, show=True): # '64'x128x1x1 如果 batch_size為64 ( 值正態分佈 )\n",
        "    fake_images = generator(latent_tensors)         # '64'x3x64x64                    ( -1< 值 <1 )\n",
        "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)    # ( 0< 值 <1 )\n",
        "\n",
        "    print('Saving', fake_fname)\n",
        "    if show:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))         # 沒有denorm\n",
        "        # ax.imshow(make_grid(denorm(fake_images.cpu().detach()), nrow=8).permute(1, 2, 0)) # 有denorm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQNysyNY7rPg"
      },
      "source": [
        "We'll use a fixed set of input vectors to the generator to see how the individual generated images evolve over time as we train the model. Let's save one set of images before we start training our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T09:58:11.883716Z",
          "iopub.status.busy": "2023-03-06T09:58:11.883272Z",
          "iopub.status.idle": "2023-03-06T09:58:11.891051Z",
          "shell.execute_reply": "2023-03-06T09:58:11.889226Z",
          "shell.execute_reply.started": "2023-03-06T09:58:11.883684Z"
        },
        "id": "PQN4Hvkm7rPg"
      },
      "outputs": [],
      "source": [
        "fixed_latent = torch.randn(64,latent_size, 1, 1, device=device)\n",
        "    # 產生 函數 save_samples 中所需要的 latent_tensors 64x128x1x1 值=正態分佈\n",
        "    # device (torch.device, optional) – the desired device of returned tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6caQ7csS7Lr"
      },
      "outputs": [],
      "source": [
        "# 也可以這麼寫\n",
        "# size = (64,latent_size, 1, 1)\n",
        "# fixed_latent = torch.randn(*size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-30T07:29:35.080033Z",
          "iopub.status.busy": "2023-01-30T07:29:35.078991Z",
          "iopub.status.idle": "2023-01-30T07:29:35.087827Z",
          "shell.execute_reply": "2023-01-30T07:29:35.086824Z",
          "shell.execute_reply.started": "2023-01-30T07:29:35.079981Z"
        },
        "id": "UDUU7AwZS7Ls"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# fixed_latent.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T09:58:34.772364Z",
          "iopub.status.busy": "2023-03-06T09:58:34.771859Z",
          "iopub.status.idle": "2023-03-06T09:58:35.541189Z",
          "shell.execute_reply": "2023-03-06T09:58:35.539768Z",
          "shell.execute_reply.started": "2023-03-06T09:58:34.772326Z"
        },
        "id": "gegjkAiFS7Lt"
      },
      "outputs": [],
      "source": [
        "save_samples(0, fixed_latent) # 有denorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T10:13:44.292877Z",
          "iopub.status.busy": "2023-01-31T10:13:44.291698Z",
          "iopub.status.idle": "2023-01-31T10:13:44.964184Z",
          "shell.execute_reply": "2023-01-31T10:13:44.963085Z",
          "shell.execute_reply.started": "2023-01-31T10:13:44.292827Z"
        },
        "id": "VLZaqP0T7rPh"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# save_samples(0, fixed_latent)  # 沒有denorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T08:49:13.643609Z",
          "iopub.status.busy": "2023-01-31T08:49:13.643186Z",
          "iopub.status.idle": "2023-01-31T08:49:13.668522Z",
          "shell.execute_reply": "2023-01-31T08:49:13.667014Z",
          "shell.execute_reply.started": "2023-01-31T08:49:13.643570Z"
        },
        "id": "PJ8IIzkSS7Lw"
      },
      "outputs": [],
      "source": [
        "# 測試用\n",
        "idx = 1\n",
        "test_fake_images = generator(fixed_latent)\n",
        "test_fake_fname = 'generated-images-{0:0>4d}.png'.format(idx)\n",
        "print(test_fake_fname)\n",
        "# save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=8)\n",
        "# print('Saving', fake_fname)\n",
        "\n",
        "\n",
        "# fig, ax = plt.subplots(figsize=(8, 8))\n",
        "# ax.set_xticks([]); ax.set_yticks([])\n",
        "# ax.imshow(make_grid(test_fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.set_xticks([]); ax.set_yticks([])\n",
        "ax.imshow(test_fake_images[0].detach().permute(1, 2, 0))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 8)) # plt.subplots(2, 3) 即表示一次性在figure上創建成2*3的網格\n",
        "ax.set_xticks([]); ax.set_yticks([])\n",
        "ax.imshow(denorm(test_fake_images.detach()[0]).permute(1, 2, 0))\n",
        "\n",
        "# torch.Tensor.cpu 會回傳 a copy of this object in CPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-30T12:41:32.023728Z",
          "iopub.status.busy": "2023-01-30T12:41:32.023262Z",
          "iopub.status.idle": "2023-01-30T12:41:32.037188Z",
          "shell.execute_reply": "2023-01-30T12:41:32.035709Z",
          "shell.execute_reply.started": "2023-01-30T12:41:32.023693Z"
        },
        "id": "8T3AONGnS7Ly"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "test_fake_images[0] # -1 < 值 < 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-30T12:41:56.350142Z",
          "iopub.status.busy": "2023-01-30T12:41:56.349709Z",
          "iopub.status.idle": "2023-01-30T12:41:56.359672Z",
          "shell.execute_reply": "2023-01-30T12:41:56.358534Z",
          "shell.execute_reply.started": "2023-01-30T12:41:56.350082Z"
        },
        "id": "D0lDChAFS7L0"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "denorm(test_fake_images[0]) # 0 < 值 < 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHdCGVDW7rPh"
      },
      "outputs": [],
      "source": [
        "jovian.commit(project=project_name, environment=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_uc0l7r7rPh"
      },
      "source": [
        "## Full Training Loop\n",
        "\n",
        "Let's define a `fit` function to train the discriminator and generator in tandem for each batch of training data. We'll use the Adam optimizer with some custom parameters (betas) that are known to work well for GANs. We will also save some sample generated images at regular intervals for inspection.\n",
        "\n",
        "<img src=\"https://i.imgur.com/6NMdO9u.png\" style=\"max-width:420px; margin-bottom:32px\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T10:08:39.837252Z",
          "iopub.status.busy": "2023-03-06T10:08:39.836690Z",
          "iopub.status.idle": "2023-03-06T10:08:39.843477Z",
          "shell.execute_reply": "2023-03-06T10:08:39.841796Z",
          "shell.execute_reply.started": "2023-03-06T10:08:39.837211Z"
        },
        "id": "bFfiZRa37rPh"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlyaj-sLS7L8"
      },
      "source": [
        "圖檔資料集 train_ds 包含了 63565 個圖 <br>\n",
        "print(len(train_ds)) = 63565 <br>\n",
        "每一個圖是以一個tuple呈現 <br>\n",
        "type(train_ds[0]) 顯示 tuple <br>\n",
        "train_ds[0] 這個tuple中，包含一個tensor (圖檔數值資料) 與 一個int (資料的標籤)<br>\n",
        "train_ds[0][0] 是一個 3x64x64的tensor ， train_ds[0][1] 是一個 整數值 <br>\n",
        "train_ds[0][0] 中的值都介於 -1 與 1 之間"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T12:25:48.973800Z",
          "iopub.status.busy": "2023-01-31T12:25:48.972641Z",
          "iopub.status.idle": "2023-01-31T12:25:48.980868Z",
          "shell.execute_reply": "2023-01-31T12:25:48.979550Z",
          "shell.execute_reply.started": "2023-01-31T12:25:48.973762Z"
        },
        "id": "YSy1TH7qS7L9"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# for tts, lts in train_ds:\n",
        "#     print(tts.shape)\n",
        "#     print(lts)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T12:39:02.454162Z",
          "iopub.status.busy": "2023-01-31T12:39:02.452755Z",
          "iopub.status.idle": "2023-01-31T12:39:32.581250Z",
          "shell.execute_reply": "2023-01-31T12:39:32.579917Z",
          "shell.execute_reply.started": "2023-01-31T12:39:02.454109Z"
        },
        "id": "drfOp7v-S7L-"
      },
      "outputs": [],
      "source": [
        "# 測試\n",
        "# t=0\n",
        "# for real_images, lb in tqdm(train_dl):\n",
        "#     t+=1\n",
        "# print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T12:41:54.822610Z",
          "iopub.status.busy": "2023-01-31T12:41:54.821520Z",
          "iopub.status.idle": "2023-01-31T12:41:55.270272Z",
          "shell.execute_reply": "2023-01-31T12:41:55.268754Z",
          "shell.execute_reply.started": "2023-01-31T12:41:54.822548Z"
        },
        "id": "m8sBQcFVS7MA"
      },
      "outputs": [],
      "source": [
        "# 測試 real_images 的 shape\n",
        "# for real_images, lb in tqdm(train_dl):\n",
        "#     print(real_images.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T14:25:32.065023Z",
          "iopub.status.busy": "2023-01-31T14:25:32.064601Z",
          "iopub.status.idle": "2023-01-31T14:25:32.564307Z",
          "shell.execute_reply": "2023-01-31T14:25:32.562722Z",
          "shell.execute_reply.started": "2023-01-31T14:25:32.064989Z"
        },
        "id": "z09kg4DpS7MB"
      },
      "outputs": [],
      "source": [
        "# 測試 real_images 的 值\n",
        "for real_images, lb in tqdm(train_dl_g):\n",
        "    print(real_images[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-01-31T14:27:24.882001Z",
          "iopub.status.busy": "2023-01-31T14:27:24.881578Z",
          "iopub.status.idle": "2023-01-31T14:27:25.952580Z",
          "shell.execute_reply": "2023-01-31T14:27:25.950857Z",
          "shell.execute_reply.started": "2023-01-31T14:27:24.881966Z"
        },
        "id": "mSR0YpbSS7MD"
      },
      "outputs": [],
      "source": [
        "# for real_images, _ in tqdm(train_dl_g):\n",
        "#     real_preds = discriminator(real_images)                       # 128x1x1x1 (0 < 值 < 1)\n",
        "#     real_targets = torch.ones(real_images.size(0), 1, device=device)  # 128x1 (值為1)\n",
        "#     real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
        "#     real_score = torch.mean(real_preds).item()                    # 沒有item() 則會以tensor形式呈現\n",
        "#     print(real_loss)\n",
        "#     print(real_score)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T10:08:49.754104Z",
          "iopub.status.busy": "2023-03-06T10:08:49.753683Z",
          "iopub.status.idle": "2023-03-06T10:08:49.766679Z",
          "shell.execute_reply": "2023-03-06T10:08:49.764509Z",
          "shell.execute_reply.started": "2023-03-06T10:08:49.754052Z"
        },
        "id": "XHsBi5hW7rPh"
      },
      "outputs": [],
      "source": [
        "def fit(epochs, lr, start_idx=1):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Losses & scores\n",
        "    losses_g = []\n",
        "    losses_d = []\n",
        "    real_scores = []\n",
        "    fake_scores = []\n",
        "\n",
        "    # Create optimizers (在betas指定了beta_1與beta_2)\n",
        "    opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "    opt_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for real_images, _ in tqdm(train_dl_g): # real_images的shape為 128x3x64x64\n",
        "            # Train discriminator\n",
        "            loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
        "            # Train generator\n",
        "            loss_g = train_generator(opt_g)\n",
        "\n",
        "        # Record losses & scores\n",
        "        losses_g.append(loss_g)\n",
        "        losses_d.append(loss_d)\n",
        "        real_scores.append(real_score)\n",
        "        fake_scores.append(fake_score)\n",
        "\n",
        "        # Log losses & scores (last batch)\n",
        "        print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
        "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
        "\n",
        "        # Save generated images\n",
        "        save_samples(epoch+start_idx, fixed_latent, show=False)\n",
        "\n",
        "    return losses_g, losses_d, real_scores, fake_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxw7tdof7rPh"
      },
      "source": [
        "We are now ready to train the model. Try different learning rates to see if you can maintain the fine balance between the training the generator and the discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T10:15:19.892603Z",
          "iopub.status.busy": "2023-03-06T10:15:19.892166Z",
          "iopub.status.idle": "2023-03-06T10:15:19.897436Z",
          "shell.execute_reply": "2023-03-06T10:15:19.896476Z",
          "shell.execute_reply.started": "2023-03-06T10:15:19.892568Z"
        },
        "id": "KCO9Xz8a7rPi"
      },
      "outputs": [],
      "source": [
        "lr = 0.0002\n",
        "epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T13:50:28.108463Z",
          "iopub.status.busy": "2023-02-01T13:50:28.108114Z",
          "iopub.status.idle": "2023-02-01T13:50:42.181921Z",
          "shell.execute_reply": "2023-02-01T13:50:42.180858Z",
          "shell.execute_reply.started": "2023-02-01T13:50:28.108434Z"
        },
        "id": "i38VPJYD7rPi"
      },
      "outputs": [],
      "source": [
        "jovian.reset()\n",
        "jovian.log_hyperparams(lr=lr, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T13:50:49.311205Z",
          "iopub.status.busy": "2023-02-01T13:50:49.310826Z",
          "iopub.status.idle": "2023-02-01T14:35:35.613185Z",
          "shell.execute_reply": "2023-02-01T14:35:35.611844Z",
          "shell.execute_reply.started": "2023-02-01T13:50:49.311170Z"
        },
        "id": "QukXeGTw7rPi"
      },
      "outputs": [],
      "source": [
        "history = fit(epochs, lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:35:45.707145Z",
          "iopub.status.busy": "2023-02-01T14:35:45.706733Z",
          "iopub.status.idle": "2023-02-01T14:35:45.712531Z",
          "shell.execute_reply": "2023-02-01T14:35:45.711493Z",
          "shell.execute_reply.started": "2023-02-01T14:35:45.707106Z"
        },
        "id": "Uzxo0l047rPi"
      },
      "outputs": [],
      "source": [
        "losses_g, losses_d, real_scores, fake_scores = history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:35:45.720776Z",
          "iopub.status.busy": "2023-02-01T14:35:45.720454Z",
          "iopub.status.idle": "2023-02-01T14:35:45.949239Z",
          "shell.execute_reply": "2023-02-01T14:35:45.948223Z",
          "shell.execute_reply.started": "2023-02-01T14:35:45.720731Z"
        },
        "id": "GyP4YKWb7rPj"
      },
      "outputs": [],
      "source": [
        "jovian.log_metrics(loss_g=losses_g[-1],\n",
        "                   loss_d=losses_d[-1],\n",
        "                   real_score=real_scores[-1],\n",
        "                   fake_score=fake_scores[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELs5OAbH7rPj"
      },
      "source": [
        "Now that we have trained the models, we can save checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:35:51.854082Z",
          "iopub.status.busy": "2023-02-01T14:35:51.853692Z",
          "iopub.status.idle": "2023-02-01T14:35:51.919591Z",
          "shell.execute_reply": "2023-02-01T14:35:51.918443Z",
          "shell.execute_reply.started": "2023-02-01T14:35:51.854013Z"
        },
        "id": "b55q3sQR7rPj"
      },
      "outputs": [],
      "source": [
        "# Save the model checkpoints\n",
        "torch.save(generator.state_dict(), 'G.pth')\n",
        "torch.save(discriminator.state_dict(), 'D.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctJgUSgt7rPj"
      },
      "source": [
        "Here's how the generated images look, after the 1st, 5th and 10th epochs of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T10:52:43.807194Z",
          "iopub.status.busy": "2023-03-06T10:52:43.806663Z",
          "iopub.status.idle": "2023-03-06T10:52:43.813216Z",
          "shell.execute_reply": "2023-03-06T10:52:43.811927Z",
          "shell.execute_reply.started": "2023-03-06T10:52:43.807157Z"
        },
        "id": "b033aUC07rPj"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T10:17:17.314625Z",
          "iopub.status.busy": "2023-03-06T10:17:17.313643Z",
          "iopub.status.idle": "2023-03-06T10:17:17.344288Z",
          "shell.execute_reply": "2023-03-06T10:17:17.343101Z",
          "shell.execute_reply.started": "2023-03-06T10:17:17.314582Z"
        },
        "id": "lI0-wdzsS7MV"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0000.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-06T10:16:22.064246Z",
          "iopub.status.busy": "2023-03-06T10:16:22.063814Z",
          "iopub.status.idle": "2023-03-06T10:16:22.093847Z",
          "shell.execute_reply": "2023-03-06T10:16:22.092806Z",
          "shell.execute_reply.started": "2023-03-06T10:16:22.064210Z"
        },
        "id": "L4Hpc2Wm7rPj"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0001.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:36:06.899632Z",
          "iopub.status.busy": "2023-02-01T14:36:06.898929Z",
          "iopub.status.idle": "2023-02-01T14:36:06.918414Z",
          "shell.execute_reply": "2023-02-01T14:36:06.917387Z",
          "shell.execute_reply.started": "2023-02-01T14:36:06.899594Z"
        },
        "id": "Xm4Yq2O67rPj"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0005.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:36:13.914204Z",
          "iopub.status.busy": "2023-02-01T14:36:13.913823Z",
          "iopub.status.idle": "2023-02-01T14:36:13.935753Z",
          "shell.execute_reply": "2023-02-01T14:36:13.934398Z",
          "shell.execute_reply.started": "2023-02-01T14:36:13.914169Z"
        },
        "id": "us-eJzle7rPk"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0010.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:36:19.759932Z",
          "iopub.status.busy": "2023-02-01T14:36:19.759561Z",
          "iopub.status.idle": "2023-02-01T14:36:19.779124Z",
          "shell.execute_reply": "2023-02-01T14:36:19.778063Z",
          "shell.execute_reply.started": "2023-02-01T14:36:19.759900Z"
        },
        "id": "x9Jmym7z7rPk"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0020.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:36:27.934345Z",
          "iopub.status.busy": "2023-02-01T14:36:27.933966Z",
          "iopub.status.idle": "2023-02-01T14:36:27.954442Z",
          "shell.execute_reply": "2023-02-01T14:36:27.953672Z",
          "shell.execute_reply.started": "2023-02-01T14:36:27.934312Z"
        },
        "id": "m9NI8e717rPk"
      },
      "outputs": [],
      "source": [
        "Image('./generated/generated-images-0025.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp3Y77VU7rPk"
      },
      "source": [
        "We can visualize the training process by combining the sample images generated after each epoch into a video using OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T14:36:47.587164Z",
          "iopub.status.busy": "2023-02-01T14:36:47.586752Z",
          "iopub.status.idle": "2023-02-01T14:36:47.929884Z",
          "shell.execute_reply": "2023-02-01T14:36:47.928673Z",
          "shell.execute_reply.started": "2023-02-01T14:36:47.587128Z"
        },
        "id": "bfABlRRl7rPk"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "vid_fname = 'gans_training.avi'\n",
        "\n",
        "files = [os.path.join(sample_dir, f) for f in os.listdir(sample_dir) if 'generated' in f]\n",
        "files.sort()\n",
        "\n",
        "out = cv2.VideoWriter(vid_fname,cv2.VideoWriter_fourcc(*'MP4V'), 1, (530,530))\n",
        "[out.write(cv2.imread(fname)) for fname in files]\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9OFv0oj7rPk"
      },
      "source": [
        "Here's what it looks like:\n",
        "\n",
        "![]()\n",
        "\n",
        "\n",
        "We can also visualize how the loss changes over time. Visualizing\n",
        "losses is quite useful for debugging the training process. For GANs, we expect the generator's loss to reduce over time, without the discriminator's loss getting too high.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-01T14:37:13.783721Z"
        },
        "id": "lLdS3gb87rPk"
      },
      "outputs": [],
      "source": [
        "plt.plot(losses_d, '-')\n",
        "plt.plot(losses_g, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['Discriminator', 'Generator'])\n",
        "plt.title('Losses');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McZ165r67rPl"
      },
      "outputs": [],
      "source": [
        "plt.plot(real_scores, '-')\n",
        "plt.plot(fake_scores, '-')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('score')\n",
        "plt.legend(['Real', 'Fake'])\n",
        "plt.title('Scores');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaHPbcUr7rPl"
      },
      "source": [
        "## Save and Commit\n",
        "\n",
        "We can upload the full snapshot of this experiment to Jovian:\n",
        "- Jupyter notebook\n",
        "- Hyperparameters & metrics\n",
        "- Models weights\n",
        "- Training video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSyX2Y647rPl"
      },
      "outputs": [],
      "source": [
        "import jovian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FIXjExB7rPl"
      },
      "outputs": [],
      "source": [
        "jovian.commit(project=project_name,\n",
        "              outputs=['G.pth', 'D.pth', 'gans_training.avi'],\n",
        "              environment=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyi3O6QaS7M6"
      },
      "source": [
        "# <font color=\"red\">參考</font>\n",
        "\n",
        "* ## <font color=\"red\">[反捲積 Transposed convolution](https://zhuanlan.zhihu.com/p/124626648) </font>\n",
        "> [A guide to convolution arithmetic for deep\n",
        "learning](https://arxiv.org/pdf/1603.07285.pdf)\n",
        "\n",
        "* ## <font color=\"red\">[state_dict() 說明](https://ithelp.ithome.com.tw/articles/10279104)</font>\n",
        "\n",
        "* ## <font color=\"red\">[The math behind GANs (Generative Adversarial Networks)](https://towardsdatascience.com/the-math-behind-gans-generative-adversarial-networks-3828f3469d9c)</font>\n",
        "\n",
        "* ## <font color=\"red\">[生成對抗網路](https://www.twblogs.net/a/5f031dd0e94612e3fcea95f6)</font>\n",
        "\n",
        "* ## <font color=\"red\">[Theory behind GAN(台大)](http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/GANtheory%20(v2).pdf)</font>\n",
        "\n",
        "* ## <font color=\"red\">[懂Wasserstein Distance嗎?看看這篇](https://chih-sheng-huang821.medium.com/wasserstein-distance-b3c33d4b942)</font>\n",
        "\n",
        "* ## <font color=\"red\">[DCGAN](https://ithelp.ithome.com.tw/articles/10303526)</font>\n",
        "\n",
        "* ## <font color=\"red\">[Entropy, Cross Entropy和KL-Divergence](https://meetonfriday.com/posts/216f7e20/)</font>\n",
        "\n",
        "* ## <font color=\"red\">[關於GAN的一些筆記（參照Dr. Lee的上課內容）](https://www.cnblogs.com/dilthey/p/12332459.html)</font>\n",
        "\n",
        "# <font color=\"red\">[save_image 函數保存空白圖像問題](https://blog.csdn.net/xiaoqiaoliushuiCC/article/details/112916327)</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T11:52:22.848324Z",
          "iopub.status.busy": "2023-02-01T11:52:22.847924Z",
          "iopub.status.idle": "2023-02-01T11:52:22.854698Z",
          "shell.execute_reply": "2023-02-01T11:52:22.853383Z",
          "shell.execute_reply.started": "2023-02-01T11:52:22.848292Z"
        },
        "id": "0wlQ1bKSS7Mp"
      },
      "outputs": [],
      "source": [
        "created_root = 'imgfolder'\n",
        "created_dir = 'imgfolder/images'\n",
        "filename_path = os.path.join(created_dir,'test.png')\n",
        "os.makedirs(created_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T12:14:24.718589Z",
          "iopub.status.busy": "2023-02-01T12:14:24.718145Z",
          "iopub.status.idle": "2023-02-01T12:14:24.731934Z",
          "shell.execute_reply": "2023-02-01T12:14:24.730629Z",
          "shell.execute_reply.started": "2023-02-01T12:14:24.718553Z"
        },
        "id": "JtBNVc02S7Mq"
      },
      "outputs": [],
      "source": [
        "for imgx, _ in train_ds: # 有Normalize過\n",
        "    print(imgx)\n",
        "    break;\n",
        "save_image(imgx, filename_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T12:26:18.751876Z",
          "iopub.status.busy": "2023-02-01T12:26:18.751427Z",
          "iopub.status.idle": "2023-02-01T12:26:19.062784Z",
          "shell.execute_reply": "2023-02-01T12:26:19.061502Z",
          "shell.execute_reply.started": "2023-02-01T12:26:18.751843Z"
        },
        "id": "c6f7WNzhS7Mr"
      },
      "outputs": [],
      "source": [
        "t1_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n",
        "    T.Resize(image_size),\n",
        "    T.CenterCrop(image_size),\n",
        "    T.ToTensor()]))     # 沒有Normalize\n",
        "\n",
        "for t1_01, _ in t1_ds:  # 沒有Normalize\n",
        "    print(t1_01)\n",
        "    break\n",
        "\n",
        "filename_n_path = os.path.join(created_dir,'test_n.png')\n",
        "save_image(t1_01, filename_n_path, Normalize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T11:53:33.022764Z",
          "iopub.status.busy": "2023-02-01T11:53:33.021670Z",
          "iopub.status.idle": "2023-02-01T11:53:33.027996Z",
          "shell.execute_reply": "2023-02-01T11:53:33.026854Z",
          "shell.execute_reply.started": "2023-02-01T11:53:33.022723Z"
        },
        "id": "IoFdJzLwS7Mt"
      },
      "outputs": [],
      "source": [
        "# 把指定路徑的folder與裡面所包含的檔案 都刪除\n",
        "# import shutil\n",
        "# shutil.rmtree(created_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T13:17:05.356366Z",
          "iopub.status.busy": "2023-02-01T13:17:05.355979Z",
          "iopub.status.idle": "2023-02-01T13:17:05.365313Z",
          "shell.execute_reply": "2023-02-01T13:17:05.364016Z",
          "shell.execute_reply.started": "2023-02-01T13:17:05.356333Z"
        },
        "id": "am6ogwqdS7Mv"
      },
      "outputs": [],
      "source": [
        "# 讀取rgb格式的圖片\n",
        "img = Image.open(os.path.join(created_dir,'test.png'))  # 有Normalize\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T13:17:11.349565Z",
          "iopub.status.busy": "2023-02-01T13:17:11.349114Z",
          "iopub.status.idle": "2023-02-01T13:17:11.360031Z",
          "shell.execute_reply": "2023-02-01T13:17:11.359145Z",
          "shell.execute_reply.started": "2023-02-01T13:17:11.349513Z"
        },
        "id": "WIIh0gcsS7Mw"
      },
      "outputs": [],
      "source": [
        "img_n = Image.open(os.path.join(created_dir,'test_n.png')) # 沒有Normalize\n",
        "img_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T12:37:56.073575Z",
          "iopub.status.busy": "2023-02-01T12:37:56.073133Z",
          "iopub.status.idle": "2023-02-01T12:37:56.082366Z",
          "shell.execute_reply": "2023-02-01T12:37:56.081529Z",
          "shell.execute_reply.started": "2023-02-01T12:37:56.073525Z"
        },
        "id": "PyjJbqUQS7My"
      },
      "outputs": [],
      "source": [
        "# (h,w,c)--(c,h,w)，pytorch的輸入格式必需為(c,h,w)。numpy.ndarray不能用permute要用transpose\n",
        "imgarr = np.array(img).transpose((2,0,1))\n",
        "# print(imgarr.shape)   # (3, 64, 64)\n",
        "print(imgarr/255) # imgarr 還沒有歸一化所以除以255，就可以跟上面imgx的比較，基本上如果是正值會一樣\n",
        "imgascon = np.ascontiguousarray([imgarr,imgarr,imgarr,imgarr])\n",
        "# print(imgascon.shape)  # (4, 3, 64, 64)\n",
        "imgts = torch.Tensor(imgascon)\n",
        "# print(imgts.shape)  # torch.Size([4, 3, 64, 64])\n",
        "\n",
        "# 注意imgts沒有負值，但確有很多0，似乎在save_image中輸入的imgx有負值，但save_image會把負值改成0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T12:41:36.699245Z",
          "iopub.status.busy": "2023-02-01T12:41:36.698811Z",
          "iopub.status.idle": "2023-02-01T12:41:36.709884Z",
          "shell.execute_reply": "2023-02-01T12:41:36.708423Z",
          "shell.execute_reply.started": "2023-02-01T12:41:36.699209Z"
        },
        "id": "PI644xH1S7Mz"
      },
      "outputs": [],
      "source": [
        "imgarr_n = np.array(img_n).transpose((2,0,1))\n",
        "\n",
        "# imgascon = np.ascontiguousarray([imgarr,imgarr,imgarr,imgarr])\n",
        "# print(imgascon.shape)  # (4, 3, 64, 64)\n",
        "imgnts = torch.Tensor(imgarr_n/255)\n",
        "print(imgnts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T11:55:56.074886Z",
          "iopub.status.busy": "2023-02-01T11:55:56.074449Z",
          "iopub.status.idle": "2023-02-01T11:55:56.081260Z",
          "shell.execute_reply": "2023-02-01T11:55:56.080021Z",
          "shell.execute_reply.started": "2023-02-01T11:55:56.074844Z"
        },
        "id": "bPuYIglNS7M1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# 读取rgb格式的图片\n",
        "# img = Image.open(\"3.jpg\")\n",
        "# (h,w,c)--(c,h,w)，pytorch的输入格式必须为(c,h,w)\n",
        "# img = np.array(img).transpose((2,0,1))\n",
        "# 执行了transpose后，numpy数组的内存不连续了，转换到tensor时会报错，需要先执行如下操作\n",
        "# img = np.ascontiguousarray([img,img,img,img])\n",
        "# img = torch.Tensor(img)\n",
        "\n",
        "# 以下两句代码可以注释，save_image()函数里已经包含了make_grid()操作\n",
        "# img_grid = torchvision.utils.make_grid(img)\n",
        "# print(img_grid.shape)\n",
        "\n",
        "# img如果没有归一化，必须要除以255。\n",
        "# torchvision.utils.save_image(img/255.0,\"test.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T12:05:56.764003Z",
          "iopub.status.busy": "2023-02-01T12:05:56.763572Z",
          "iopub.status.idle": "2023-02-01T12:05:56.770041Z",
          "shell.execute_reply": "2023-02-01T12:05:56.768856Z",
          "shell.execute_reply.started": "2023-02-01T12:05:56.763967Z"
        },
        "id": "uMNZB1IoS7M2"
      },
      "outputs": [],
      "source": [
        "# 這裡用ImageFolder時，目錄的選項要用created_root，不能用created_dir，否則會找不到image class folder\n",
        "created_ds = ImageFolder(created_root, transform=T.Compose([\n",
        "    T.ToTensor()]))  # 注意這裡沒使用normalize，所以沒有負值"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T12:06:00.139385Z",
          "iopub.status.busy": "2023-02-01T12:06:00.138599Z",
          "iopub.status.idle": "2023-02-01T12:06:00.151162Z",
          "shell.execute_reply": "2023-02-01T12:06:00.149757Z",
          "shell.execute_reply.started": "2023-02-01T12:06:00.139346Z"
        },
        "id": "CrbFWb5CS7M3"
      },
      "outputs": [],
      "source": [
        "for imgx1, _ in created_ds:\n",
        "    print(imgx1)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-02-01T12:15:51.439914Z",
          "iopub.status.busy": "2023-02-01T12:15:51.438870Z",
          "iopub.status.idle": "2023-02-01T12:15:51.742932Z",
          "shell.execute_reply": "2023-02-01T12:15:51.741897Z",
          "shell.execute_reply.started": "2023-02-01T12:15:51.439874Z"
        },
        "id": "wQFuImB9S7M5"
      },
      "outputs": [],
      "source": [
        "t1_ds = ImageFolder(DATA_DIR, transform=T.Compose([\n",
        "    T.Resize(image_size),\n",
        "    T.CenterCrop(image_size),\n",
        "    T.ToTensor()]))  # 沒有Normalize\n",
        "\n",
        "for t1_01, _ in t1_ds: # 沒有Normalize\n",
        "    print(t1_01)\n",
        "    break"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}