{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KBsFIaY09WrR",
        "nj60i4G5EDKd",
        "kl05kiNlGgkB"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrykohl/Machine-Learning-demo-repo/blob/master/case-study/grad01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [葉子張量和非葉子張量](https://www.modb.pro/db/542980)\n",
        "連結已失效"
      ],
      "metadata": {
        "id": "bDxg1mCSQJBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 1-1"
      ],
      "metadata": {
        "id": "KBsFIaY09WrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 此時w1是葉子張量\n",
        "w1 = torch.ones(10, requires_grad=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  w1 *= 0.5\n",
        "\n",
        "\n",
        "w2 = torch.ones(10, requires_grad=True)\n",
        "\n",
        "wto = torch.exp(w1)*w2\n",
        "\n",
        "loss = wto.sum()\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "Td__3aKiQNBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print(w1.is_leaf)\n",
        "print(w2.is_leaf)\n",
        "# w1 與 w2 都是葉節點\n",
        "\n",
        "print(w1.grad)\n",
        "print(w2.grad)\n",
        "```\n",
        "True\n",
        "\n",
        "True\n",
        "\n",
        "tensor([1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487,\n",
        "        1.6487])\n",
        "\n",
        "tensor([1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487,\n",
        "        1.6487])"
      ],
      "metadata": {
        "id": "424x_zk38wf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wpuLfL3g9xpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 1-2\n",
        "\n"
      ],
      "metadata": {
        "id": "JrR_hj3G-DL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 可改成 (此時w1不是葉子張量)\n",
        "w1 = torch.ones(10, requires_grad=True)*0.5\n",
        "\n",
        "w2 = torch.ones(10, requires_grad=True)\n",
        "\n",
        "# wto = torch.exp(w1)*w2\n",
        "\n",
        "# loss = wto.sum()\n",
        "# loss.backward()"
      ],
      "metadata": {
        "id": "h8dmQ50h7f1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w1.is_leaf) # w1 是非葉節點\n",
        "print(w2.is_leaf) # w2 是葉節點\n",
        "\n",
        "print(w1.grad) # 非葉子張量，求導結果不會保留，所以會有警告\n",
        "print(w2.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8UToqzMF0FZ",
        "outputId": "0404c7b0-b866-4757-9d9a-7be0f2ce0592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-487a018ae770>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(w1.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "print(w1.is_leaf) # w1 是非葉節點\n",
        "print(w2.is_leaf) # w2 是葉節點\n",
        "\n",
        "print(w1.grad)\n",
        "print(w2.grad)\n",
        "```\n",
        "False\n",
        "\n",
        "True\n",
        "\n",
        "None\n",
        "\n",
        "tensor([1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487,\n",
        "        1.6487])\n",
        "\n",
        "...\n"
      ],
      "metadata": {
        "id": "Minnyey0_MQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [詳解Pytorch中的requires_grad、葉子節點與非葉子節點](https://blog.csdn.net/qq_36429555/article/details/118657440)"
      ],
      "metadata": {
        "id": "RAChECs_CUNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 2-1"
      ],
      "metadata": {
        "id": "nj60i4G5EDKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "\n",
        "b=a+1 # 不是葉子\n",
        "\n",
        "print(a.is_leaf)\n",
        "print(a.requires_grad) # 明確設定是True\n",
        "print(b.is_leaf)\n",
        "print(b.requires_grad) #\n",
        "\n",
        "\n",
        "b.retain_grad() # b不是葉子，反向求導時若要使用.grad去顯示，必需要使用retain_grad()\n",
        "b.backward()\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_5IFGDK_C2S",
        "outputId": "33579287-d9c4-4303-fe01-dfad6875bcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "False\n",
            "True tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.tensor([1.0]) # 是葉子\n",
        "\n",
        "b=a+1 # 依然是葉子，因為不需要對a求導，所以反向求導時，a節點變得沒有意義\n",
        "\n",
        "print(a.is_leaf)\n",
        "print(a.requires_grad) # 是False(沒設定是True)\n",
        "print(b.is_leaf)    # 注意，是True\n",
        "print(b.requires_grad) #\n",
        "\n",
        "# 注意，b是葉子，所以b不能調用backward()，此例子很特殊，因為沒有非葉子節點"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xyXt2JXFiMC",
        "outputId": "679a5a3b-9317-4f17-8420-d507b0c6d8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 2-2\n",
        "https://blog.csdn.net/qq_27825451/article/details/95498211\n",
        "\n",
        "http://nysdy.com/post/pytorch_detach_data/"
      ],
      "metadata": {
        "id": "Z5DYpdevmlys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "\n",
        "b=a.sigmoid()\n",
        "# b=a+1 # 不同的運算式\n",
        "\n",
        "c = b.detach()\n",
        "\n",
        "\"\"\"與detach有關\"\"\"\n",
        "# c.requires_grad = True\n",
        "# c.sum().backward() # 如果沒加上一行，單單這一行會出現錯誤，用這個分離出來的tensor去求導，會影響backward()，所以出現錯誤\n",
        "\n",
        "\"\"\"與detach無關\"\"\"\n",
        "b.sum().backward()\n",
        "\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOuhAIjf5vmw",
        "outputId": "d0887fe5-f858-419e-b88b-fa8e49f2e966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1966])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"錯誤示範\"\"\"\n",
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "print(a.grad)\n",
        "b=a.sigmoid() # 注意這裡是用.sigmoid(), 如果b=a+1，後面的c進行in place操作，並不會出錯(Why?)\n",
        "print(b)\n",
        "\n",
        "c = b.detach()\n",
        "print(c)\n",
        "c.zero_() # 使用in-place操作，對c修改，這會會影響backward()，所以出現錯誤\n",
        "\n",
        "print(c)\n",
        "print(b)\n",
        "\n",
        "b.sum().backward()\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "pDExDg2mmfHs",
        "outputId": "b024ab34-ec4b-4517-8a0b-096c9cd2ff28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([0.7311], grad_fn=<SigmoidBackward0>)\n",
            "tensor([0.7311])\n",
            "tensor([0.])\n",
            "tensor([0.], grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-1f788943111b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\".detach改用.data，backward不會出錯，因為.data後的修改不會被autograd追蹤，但卻得到錯誤的backward\"\"\"\n",
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "print(a.grad)\n",
        "b=a.sigmoid()\n",
        "print(b)\n",
        "\n",
        "c = b.data # 改變c，autograd不會追蹤到，所以不會報錯\n",
        "print(c)\n",
        "c.zero_()\n",
        "\n",
        "print(c)\n",
        "print(b)\n",
        "\n",
        "b.sum().backward()\n",
        "\n",
        "print(a.grad) # 結果被影影到\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JS6hwkFmeJH",
        "outputId": "d47e562a-8516-44a1-c9be-3225253d89cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([0.7311], grad_fn=<SigmoidBackward0>)\n",
            "tensor([0.7311])\n",
            "tensor([0.])\n",
            "tensor([0.], grad_fn=<SigmoidBackward0>)\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"改變sum到c.zero_()之前，backward結果就會是正確的\"\"\"\n",
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "print(a.grad)\n",
        "b=a.sigmoid().sum() # sum移到此\n",
        "print(b)\n",
        "\n",
        "c = b.data\n",
        "print(c)\n",
        "c.zero_()\n",
        "\n",
        "print(c)\n",
        "print(b)\n",
        "\n",
        "b.backward()\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-OcnBg8QQVV",
        "outputId": "4d279edd-dd95-4bfe-f1aa-dc4448d62984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor(0.7311, grad_fn=<SumBackward0>)\n",
            "tensor(0.7311)\n",
            "tensor(0.)\n",
            "tensor(0., grad_fn=<SumBackward0>)\n",
            "tensor([0.1966])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [PyTorch的Tensor和自动求导](https://lulaoshi.info/machine-learning/neural-network/pytorch-tensor-autograd)\n"
      ],
      "metadata": {
        "id": "kl05kiNlGgkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvbksGlJGqT8",
        "outputId": "b8bc42f9-8994-4eeb-92be-dea184277573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.backward() # 反向求導"
      ],
      "metadata": {
        "id": "MZMXLluIGxL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)  # 葉節點，有求導值\n",
        "print(y.grad)  # 非葉節點，求導值不保留，所以是None\n",
        "print(z.grad)  # 非葉節點，求導值不保留，所以是None\n",
        "print(out.grad) # 非葉節點，求導值不保留，所以是None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kKGu3TWG2gm",
        "outputId": "5e30a327-f70e-4a6a-ecf5-96cad7d140e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-68aa1dcaf5e5>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(y.grad)\n",
            "<ipython-input-12-68aa1dcaf5e5>:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(z.grad)\n",
            "<ipython-input-12-68aa1dcaf5e5>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(out.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "如果在非葉節點使用retain_grad()\n",
        "```python\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "y.retain_grad()\n",
        "z.retain_grad()\n",
        "out.retain_grad()\n",
        "\n",
        "out.backward()\n",
        "\n",
        "print(x.grad)  \n",
        "print(y.grad)   \n",
        "print(z.grad)  \n",
        "print(out.grad)\n",
        "```\n",
        "tensor([[4.5000, 4.5000], <br>\n",
        "     [4.5000, 4.5000]])\n",
        "\n",
        "tensor([[4.5000, 4.5000], <br>\n",
        "        [4.5000, 4.5000]])\n",
        "\n",
        "tensor([[0.2500, 0.2500], <br>\n",
        "        [0.2500, 0.2500]])\n",
        "        \n",
        "tensor(1.)\n"
      ],
      "metadata": {
        "id": "LvR8eiw-dBim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ujNJ6MXRG-nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_pCsu3mXnOX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}