{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KBsFIaY09WrR",
        "nj60i4G5EDKd",
        "kl05kiNlGgkB"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULMwfYNjXj5U"
      },
      "outputs": [],
      "source": [
        "class A(object):\n",
        "  a = 'a'\n",
        "\n",
        "  def foo1(name):\n",
        "    print('hello', name)\n",
        "\n",
        "  def foo2(self, name):\n",
        "    print('hello!', name)\n",
        "\n",
        "  def foo3(cls, name):\n",
        "    print('hello!!', name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Student():\n",
        "    cnt = 0 # 這個變數是屬於整個Student類別的\n",
        "    def __init__(self, name, score):\n",
        "        Student.cnt += 1 # 每次新開一個Student的物件，計數器就會+1\n",
        "        self.name = name\n",
        "        self.score = score\n",
        "\n",
        "    def readMyName(self):\n",
        "        print('聽清楚了，我的名字是' + self.name + '!!!')\n",
        "\n",
        "    def compare(self, b):\n",
        "        diff = sum(self.score.values()) - sum(b.score.values())\n",
        "        # 有冒號的式子如果底下程式碼只有一行，也可以選擇直接和判斷式寫成同一行\n",
        "        if diff > 0: print(self.name + '贏了！')\n",
        "        elif diff == 0: print('什麼？竟然平手？！')\n",
        "        else: print('可...可惡，難道，這就是' + b.name + '真正的實力嗎？')\n",
        "\n",
        "    @classmethod\n",
        "    def getCount(cls):\n",
        "        print('目前的學生總數：%d' % cls.cnt)"
      ],
      "metadata": {
        "id": "7WwRUB8OaDPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Student.getCount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS7nkDDJ207N",
        "outputId": "632d1bfd-586e-4c26-c240-2334eed8a3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "目前的學生總數：0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ming = Student('阿明', {'數學':55, '英文':70, '物理':55})\n",
        "ming.readMyName()\n",
        "Student.getCount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSQ7vmAJ24WL",
        "outputId": "4c470623-e11a-47d9-bfb4-e8a8d4e9ce02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "聽清楚了，我的名字是阿明!!!\n",
            "目前的學生總數：1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ming.score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIxW7wua3oxh",
        "outputId": "55743613-a0b7-4018-fac2-42fe4bb7297c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'數學': 55, '英文': 70, '物理': 55}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mei = Student('小美', {'數學':90, '英文':88, '物理':100})\n",
        "\n",
        "howhow = Student('HowHow', {'數學':80, '英文':60, '物理':40})\n",
        "\n",
        "Student.getCount()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHvqabZU33aR",
        "outputId": "710b700b-6ca8-49d6-cea4-e22c9a9df433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "目前的學生總數：3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Desolve():\n",
        "    @staticmethod\n",
        "    def ads():\n",
        "        print('無情工商時間!')\n",
        "        print('從LeetCode學演算法是一系列非常好的課程!')\n",
        "        print('https://bit.ly/leetcodeall')\n",
        "\n",
        "Desolve.ads()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_BZAMvj7GVx",
        "outputId": "34ac2daa-edd5-4726-a871-7a732d320ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "無情工商時間!\n",
            "從LeetCode學演算法是一系列非常好的課程!\n",
            "https://bit.ly/leetcodeall\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class nmod3():\n",
        "    def __init__(self, num):\n",
        "        self.num = num\n",
        "\n",
        "    def __eq__(self, n2): # 定義 \"==\"這個運算子為是否除以3的餘數相同\n",
        "        return self.num % 3 == n2.num % 3\n",
        "\n",
        "a = nmod3(11) # 除以3餘2\n",
        "b = nmod3(18) # 除以3餘0\n",
        "c = nmod3(17) # 除以3餘2\n",
        "\n",
        "print(a == b)\n",
        "print(a == c)\n",
        "print(b == c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ICIezp-94Nb",
        "outputId": "fdc69eeb-4310-449a-d8f6-8f04f3c31923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVflEbNPAdqy",
        "outputId": "b4a84a4b-fdcb-413b-97dc-6b212859afb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.nmod3 object at 0x7d3418224e80>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kk.py\n",
        "__all__=('A','func')\n",
        "class A():\n",
        "  def __init__(self,name,age):\n",
        "    self.name=name\n",
        "    self.age=age\n",
        "class B():\n",
        "  def __init__(self,name,id):\n",
        "    self.name=name\n",
        "    self.id=id\n",
        "def func():\n",
        "  print( 'func() is called! ')\n",
        "def func1():\n",
        "  print( 'func1() is called! ')"
      ],
      "metadata": {
        "id": "clD_I4M8AnGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "904b4ecb-9af1-4a6c-b720-3fb11ac9813a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting kk.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kk import * #由於kk.py中沒有定義__all__屬性，所以匯入了kk.py中所有的公有屬性、方法、類\n",
        "a=A('python','24')\n",
        "print(a.name,a.age)\n",
        "b=B('python',123456)\n",
        "print(b.name,b.id)\n",
        "func()\n",
        "func1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlNxSPRhWBXx",
        "outputId": "17ce1cf1-2968-431f-bacd-c80b359ef915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 24\n",
            "python 123456\n",
            "func() is called! \n",
            "func1() is called! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile kk1.py\n",
        "import sys # 真的需要建立一個sys的包或檔案\n",
        "__all__ = [\"func\",\"sys\"]\n",
        "def func():\n",
        "  print( 'func() is called!' )\n",
        "def _func():\n",
        "  print( '_func() is called!')\n",
        "def __func():\n",
        "  print( '__func() is called!' )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QOlg12qZ-8V",
        "outputId": "9e4bb9b6-a551-4375-e6f9-0e7879e80648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting kk1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test_kk.py\n",
        "from kk1 import func,_func,__func #可以通過這種方式匯入public,protected,private\n",
        "func()\n",
        "_func() #NameError: name '_func' is not defined\n",
        "__func() #NameError: name '__func' is not defined"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRVUtl2uWohp",
        "outputId": "c8ca2f20-a98b-45c4-9304-5bb0120c70b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "func() is called!\n",
            "_func() is called!\n",
            "__func() is called!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Modelt():\n",
        "  def __init__(self,name,age):\n",
        "    self.name=name\n",
        "    self.age=age\n",
        "\n",
        "  # def __str__(self):\n",
        "  #   return self.name+str(self.age)"
      ],
      "metadata": {
        "id": "z3Itp5PQpB2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "md = Modelt('Henry Hsu',35)"
      ],
      "metadata": {
        "id": "mpmPD18KqtEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(md)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-oksR05q0zH",
        "outputId": "dcd25111-eb0d-4b88-c1f3-2062876bcff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Henry Hsu35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(md)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COnGQzvFq2AV",
        "outputId": "e332b1f0-53c7-4525-e1f8-29b42334ace1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.Modelt object at 0x7de4b18ce770>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kkOLz_dEq-3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [葉子張量和非葉子張量](https://www.modb.pro/db/542980)\n",
        "連結已失效"
      ],
      "metadata": {
        "id": "bDxg1mCSQJBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 1-1"
      ],
      "metadata": {
        "id": "KBsFIaY09WrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# 此時w1是葉子張量\n",
        "w1 = torch.ones(10, requires_grad=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  w1 *= 0.5\n",
        "\n",
        "\n",
        "w2 = torch.ones(10, requires_grad=True)\n",
        "\n",
        "wto = torch.exp(w1)*w2\n",
        "\n",
        "loss = wto.sum()\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "Td__3aKiQNBl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "print(w1.is_leaf)\n",
        "print(w2.is_leaf)\n",
        "# w1 與 w2 都是葉節點\n",
        "\n",
        "print(w1.grad)\n",
        "print(w2.grad)\n",
        "```\n",
        "True\n",
        "\n",
        "True\n",
        "\n",
        "tensor([1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487,\n",
        "        1.6487])\n",
        "\n",
        "tensor([1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487,\n",
        "        1.6487])"
      ],
      "metadata": {
        "id": "424x_zk38wf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wpuLfL3g9xpS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 1-2\n",
        "\n"
      ],
      "metadata": {
        "id": "JrR_hj3G-DL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 可改成 (此時w1不是葉子張量)\n",
        "w1 = torch.ones(10, requires_grad=True)*0.5\n",
        "\n",
        "w2 = torch.ones(10, requires_grad=True)\n",
        "\n",
        "# wto = torch.exp(w1)*w2\n",
        "\n",
        "# loss = wto.sum()\n",
        "# loss.backward()"
      ],
      "metadata": {
        "id": "h8dmQ50h7f1X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w1.is_leaf) # w1 是非葉節點\n",
        "print(w2.is_leaf) # w2 是葉節點\n",
        "\n",
        "print(w1.grad)\n",
        "print(w2.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8UToqzMF0FZ",
        "outputId": "0404c7b0-b866-4757-9d9a-7be0f2ce0592"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n",
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-487a018ae770>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(w1.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "print(w1.is_leaf) # w1 是非葉節點\n",
        "print(w2.is_leaf) # w2 是葉節點\n",
        "\n",
        "print(w1.grad)\n",
        "print(w2.grad)\n",
        "```\n",
        "False\n",
        "\n",
        "True\n",
        "\n",
        "None\n",
        "\n",
        "tensor([1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487, 1.6487,\n",
        "        1.6487])\n",
        "\n",
        "...\n"
      ],
      "metadata": {
        "id": "Minnyey0_MQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [詳解Pytorch中的requires_grad、葉子節點與非葉子節點](https://blog.csdn.net/qq_36429555/article/details/118657440)"
      ],
      "metadata": {
        "id": "RAChECs_CUNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 2-1"
      ],
      "metadata": {
        "id": "nj60i4G5EDKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "\n",
        "b=a+1 # 不是葉子\n",
        "\n",
        "print(a.is_leaf)\n",
        "print(a.requires_grad) # 明確設定是True\n",
        "print(b.is_leaf)\n",
        "print(b.requires_grad) #\n",
        "\n",
        "\n",
        "b.retain_grad() # b不是葉子，反向求導時若要使用.grad去顯示，必需要使用retain_grad()\n",
        "b.backward()\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_5IFGDK_C2S",
        "outputId": "33579287-d9c4-4303-fe01-dfad6875bcf7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "False\n",
            "True tensor([1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.tensor([1.0]) # 是葉子\n",
        "\n",
        "b=a+1 # 依然是葉子，因為不需要對a求導，所以反向求導時，a節點變得沒有意義\n",
        "\n",
        "print(a.is_leaf)\n",
        "print(a.requires_grad) # 是False(沒設定是True)\n",
        "print(b.is_leaf)    # 注意，是True\n",
        "print(b.requires_grad) #\n",
        "\n",
        "# 注意，b是葉子，所以b不能調用backward()，此例子很特殊，因為沒有非葉子節點"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xyXt2JXFiMC",
        "outputId": "679a5a3b-9317-4f17-8420-d507b0c6d8f6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## case 2-2\n",
        "https://blog.csdn.net/qq_27825451/article/details/95498211\n",
        "\n",
        "http://nysdy.com/post/pytorch_detach_data/"
      ],
      "metadata": {
        "id": "Z5DYpdevmlys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "\n",
        "b=a.sigmoid()\n",
        "# b=a+1 # 不同的運算式\n",
        "\n",
        "c = b.detach()\n",
        "\n",
        "\"\"\"與detach有關\"\"\"\n",
        "# c.requires_grad = True\n",
        "# c.sum().backward() # 如果沒加上一行，單單這一行會出現錯誤，用這個分離出來的tensor去求導，會影響backward()，所以出現錯誤\n",
        "\n",
        "\"\"\"與detach無關\"\"\"\n",
        "b.sum().backward()\n",
        "\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOuhAIjf5vmw",
        "outputId": "d0887fe5-f858-419e-b88b-fa8e49f2e966"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1966])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"錯誤示範\"\"\"\n",
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "print(a.grad)\n",
        "b=a.sigmoid() # 注意這裡是用.sigmoid(), 如果b=a+1，後面的c進行in place操作，並不會出錯(Why?)\n",
        "print(b)\n",
        "\n",
        "c = b.detach()\n",
        "print(c)\n",
        "c.zero_() # 使用in-place操作，對c修改，這會會影響backward()，所以出現錯誤\n",
        "\n",
        "print(c)\n",
        "print(b)\n",
        "\n",
        "b.sum().backward()\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "pDExDg2mmfHs",
        "outputId": "b024ab34-ec4b-4517-8a0b-096c9cd2ff28"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([0.7311], grad_fn=<SigmoidBackward0>)\n",
            "tensor([0.7311])\n",
            "tensor([0.])\n",
            "tensor([0.], grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-1f788943111b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]], which is output 0 of SigmoidBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\".detach改用.data，backward不會出錯，因為.data後的修改不會被autograd追蹤，但卻得到錯誤的backward\"\"\"\n",
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "print(a.grad)\n",
        "b=a.sigmoid()\n",
        "print(b)\n",
        "\n",
        "c = b.data # 改變c，autograd不會追蹤到，所以不會報錯\n",
        "print(c)\n",
        "c.zero_()\n",
        "\n",
        "print(c)\n",
        "print(b)\n",
        "\n",
        "b.sum().backward()\n",
        "\n",
        "print(a.grad) # 結果被影影到\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JS6hwkFmeJH",
        "outputId": "d47e562a-8516-44a1-c9be-3225253d89cc"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor([0.7311], grad_fn=<SigmoidBackward0>)\n",
            "tensor([0.7311])\n",
            "tensor([0.])\n",
            "tensor([0.], grad_fn=<SigmoidBackward0>)\n",
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"改變sum到c.zero_()之前，backward結果就會是正確的\"\"\"\n",
        "a=torch.tensor([1.0], requires_grad=True) # 是葉子\n",
        "print(a.grad)\n",
        "b=a.sigmoid().sum() # sum移到此\n",
        "print(b)\n",
        "\n",
        "c = b.data\n",
        "print(c)\n",
        "c.zero_()\n",
        "\n",
        "print(c)\n",
        "print(b)\n",
        "\n",
        "b.backward()\n",
        "\n",
        "print(a.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-OcnBg8QQVV",
        "outputId": "4d279edd-dd95-4bfe-f1aa-dc4448d62984"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "tensor(0.7311, grad_fn=<SumBackward0>)\n",
            "tensor(0.7311)\n",
            "tensor(0.)\n",
            "tensor(0., grad_fn=<SumBackward0>)\n",
            "tensor([0.1966])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [PyTorch的Tensor和自动求导](https://lulaoshi.info/machine-learning/neural-network/pytorch-tensor-autograd)\n"
      ],
      "metadata": {
        "id": "kl05kiNlGgkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvbksGlJGqT8",
        "outputId": "b8bc42f9-8994-4eeb-92be-dea184277573"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out.backward() # 反向求導"
      ],
      "metadata": {
        "id": "MZMXLluIGxL8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.grad)  # 葉節點，有求導值\n",
        "print(y.grad)  # 非葉節點，求導值不保留，所以是None\n",
        "print(z.grad)  # 非葉節點，求導值不保留，所以是None\n",
        "print(out.grad) # 非葉節點，求導值不保留，所以是None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kKGu3TWG2gm",
        "outputId": "5e30a327-f70e-4a6a-ecf5-96cad7d140e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "None\n",
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-68aa1dcaf5e5>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(y.grad)\n",
            "<ipython-input-12-68aa1dcaf5e5>:3: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(z.grad)\n",
            "<ipython-input-12-68aa1dcaf5e5>:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
            "  print(out.grad)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "如果在非葉節點使用retain_grad()\n",
        "```python\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "y.retain_grad()\n",
        "z.retain_grad()\n",
        "out.retain_grad()\n",
        "\n",
        "out.backward()\n",
        "\n",
        "print(x.grad)  \n",
        "print(y.grad)   \n",
        "print(z.grad)  \n",
        "print(out.grad)\n",
        "```\n",
        "tensor([[4.5000, 4.5000], <br>\n",
        "     [4.5000, 4.5000]])\n",
        "\n",
        "tensor([[4.5000, 4.5000], <br>\n",
        "        [4.5000, 4.5000]])\n",
        "\n",
        "tensor([[0.2500, 0.2500], <br>\n",
        "        [0.2500, 0.2500]])\n",
        "        \n",
        "tensor(1.)\n"
      ],
      "metadata": {
        "id": "LvR8eiw-dBim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "ujNJ6MXRG-nC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F_pCsu3mXnOX"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}