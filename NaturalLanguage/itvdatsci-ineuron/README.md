# Data-Science-Interview-Preparation-Series

## Day 1 Python Interview Questions [Video]()

* Data Science / ML / DL / GenAI
> Python
>
> Statistics
> 
> Machine Learning
>
> Deep Learning: NLP & CV
>
> Generative AI

* Python
> Condition
>
> Loops
>
> OOPs
>
> exception handling 
>
> file handling
>
> Data stucture (in Python)
> > List
> > 
> > Tuples
> > 
> > Dict
> > 
> > String
> >
>  Function
> > ---
> > Optimization
> >
> > Memory management
> >
> > Design pattern
> > 
> > Scalable

* Statistics
> Descriptive (Data 樣態)
> 
> Inferential (Do testing -- hypothesis testing)

* Machine Learning --> Project (MLOPs) : 類似於 WebDev --> DevOps
> Pipeline (Data ingestion, **Process** (feature engineering), Model, Evaluation (loss/optimization) )
>
> Model building ： hyperparameter tuning
> > Linear regression
> > 
> > Logistic regression
> > 
> > SVM
> > 
> > Decision Tree
> > 
> > Naive bayes
> > 
> > KNN
> > 
> > Random Forest
> > 
> > XGBoost, Gradient Boost, CatBoost, AdaBoost 
> > 
> > Stacking, Boosting, Bagging

* More details about **pipleline** in ML
> Ensemble
>
> Optimization
>
> Loss
>
> Data leakage
>
> Data drift
>
> Cross validation
>
> Feature Engineeging techniques
>
> Dimension reduction
>
> Hyperparameter

* Deep Learning
> ANN
>
> CNN -- CV
>
> RNN -- NLP
>
> Reinforcement Learning
>
> GANs

* Generative AI 
> |LLM/LIM| 
> > text ---->|LLM/LIM|----> text 
> > 
> > image ---->|LLM/LIM|----> text 
> >
> > text ---->|LLM/LIM|----> image 
> > 
> > image ---->|LLM/LIM|----> image 
> |LLM/LIM| : **transformer** can do
> > CV
> > > Detection
> > > 
> > > Segmentation
> 
> RNN --> LSTM/GRU --> Attention (Encoder-Decoder) --> Self-Attention (transformer)
>> transformers --> language translation

* Summary
> Python : 20 ~ 25 quesitons
> 
> Statistics
> 
> ML / ML Project
>
> DL --> ANN, CNN, RNN, LSTM
>
> GenAI --> theory, Practical

