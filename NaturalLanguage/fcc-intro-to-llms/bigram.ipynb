{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/henrykohl/Machine-Learning-demo-repo/blob/master/NaturalLanguage/fcc-intro-to-llms/bigram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwSg8a2GaQ-d",
        "outputId": "56496023-f482-4af8-f4c0-74ac53721def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "max_iters = 1000\n",
        "# eval_interval = 2500\n",
        "learning_rate = 3e-4\n",
        "eval_iters = 250"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy1aHV7-aQ-j",
        "outputId": "dbad3e70-da0e-4838-e042-7567b70a761e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '#', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
          ]
        }
      ],
      "source": [
        "with open('wizard_of_oz.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "chars = sorted(set(text))\n",
        "print(chars)\n",
        "vocab_size = len(chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rf0Fc7CRaQ-k"
      },
      "outputs": [],
      "source": [
        "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [string_to_int[c] for c in s]           ## s 是 字串, encode 是 list\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])      ## l 是 list, decode 是 字串\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)           ## a list of values, value 大小 介於0~82\n",
        "# print(data[:100])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_hello = torch.tensor(encode('hello'))\n",
        "decoded_hello = decode(encoded_hello.tolist())\n",
        "encoded_hello.tolist(),decoded_hello"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZZ46GueX9dD",
        "outputId": "fd5aa23c-8d55-4dbc-d504-8d3a06305e4b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([63, 60, 67, 67, 70], 'hello')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZIgmmPbaQ-k",
        "outputId": "4f67aa69-bd8d-47cb-9ed8-cdd0d9426e38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "tensor([[73,  1, 54, 72,  1, 58, 75, 58],\n",
            "        [58, 69, 65, 62, 58, 57,  1, 28],\n",
            "        [56, 58, 72, 72, 11,  0,  0,  3],\n",
            "        [26, 74, 73,  1, 33,  1, 54, 66]], device='cuda:0')\n",
            "targets:\n",
            "tensor([[ 1, 54, 72,  1, 58, 75, 58, 71],\n",
            "        [69, 65, 62, 58, 57,  1, 28, 68],\n",
            "        [58, 72, 72, 11,  0,  0,  3, 40],\n",
            "        [74, 73,  1, 33,  1, 54, 66,  1]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "n = int(0.8*len(data)) ## n = 186742, 233428\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) ## 詳見下一個cell中的說明\n",
        "    \"\"\"ix 的樣態 torch.tensor([i_1, i_2, i_3, i_4])\"\"\"\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    \"\"\"x 的 shape 是 [4,8]\"\"\"\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    \"\"\"y 的 shape 是 [4,8]\"\"\"\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print('inputs:')\n",
        "# print(x.shape)\n",
        "print(x)\n",
        "print('targets:')\n",
        "print(y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "字元位置從 0 ~ len(data)-1,所以<font color=\"red\">最後**7個字元**不能選</font>\n",
        "\n",
        "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|...|-|-<font color=\"red\">|-|-|-|-|-|-|-|</font>\n",
        "\n",
        "一個block有八個字元(連續)，所以能取出block的第一個字元位置從 0 ~ len(data)-8"
      ],
      "metadata": {
        "id": "xwAkgqEqyTt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x[:t+1]\n",
        "  target  = y[t]\n",
        "  print(\"When input is\", context, 'target is', target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlarqLhCwSqx",
        "outputId": "01669a7c-8aae-457a-dc4b-c829f422b4b1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([82]) target is tensor(46)\n",
            "When input is tensor([82, 46]) target is tensor(63)\n",
            "When input is tensor([82, 46, 63]) target is tensor(60)\n",
            "When input is tensor([82, 46, 63, 60]) target is tensor(1)\n",
            "When input is tensor([82, 46, 63, 60,  1]) target is tensor(42)\n",
            "When input is tensor([82, 46, 63, 60,  1, 42]) target is tensor(73)\n",
            "When input is tensor([82, 46, 63, 60,  1, 42, 73]) target is tensor(70)\n",
            "When input is tensor([82, 46, 63, 60,  1, 42, 73, 70]) target is tensor(65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ovqBWLhaaQ-l"
      },
      "outputs": [],
      "source": [
        "\"\"\"後面會建立 model = BigramLanguageModel(vocab_size),要在model建立後 estimate_loss()才能使用\"\"\"\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters) ## losses 的 shape 是 [250]\n",
        "    \"\"\"losses 的 shape 是 250\"\"\"\n",
        "    for k in range(eval_iters):\n",
        "      X, Y = get_batch(split)\n",
        "      \"\"\"X 與 Y 的 shape 都是 [4,8]\"\"\"\n",
        "      logits, loss = model(X, Y)\n",
        "      \"\"\"logits 的 shape 是 [32,83], loss 的 shape 是 []\"\"\"\n",
        "      \"\"\"logits 在此似乎沒有功用\"\"\"\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tab = nn.Embedding(vocab_size, vocab_size)"
      ],
      "metadata": {
        "id": "QBW3ApkmsYD_"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indx = torch.randint(1,(1,1), dtype=torch.long)\n",
        "indx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdm14qwTyTTT",
        "outputId": "1d92a54d-6406-4cca-858e-6839db8b2bc0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls = torch.randn((5,3))\n",
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5YfrhXOmP6y",
        "outputId": "a4445521-cd6b-4f7c-f312-9a77af0fc4d2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0376,  1.9811, -0.4050],\n",
              "        [ 0.0323,  2.8938, -0.6420],\n",
              "        [-0.7401,  0.4022,  1.3449],\n",
              "        [ 0.5876, -0.0132, -0.5787],\n",
              "        [ 0.5533, -1.6860,  1.0879]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQRAAM0GyZVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6862KsOaQ-m",
        "outputId": "a19af11c-6b1c-4761-f890-06cc74c7b007"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "y?Y*?q8Qs]9!18WIl57vdBBV)NK\"2b257W\"RAOFN-rpwqp5tue!gYk?;Ct_Tm6j]j&E]P)u920mW7﻿JQxJ.fYSq﻿[jMB]XjyC[q7PS17lvj&﻿lH&OkKjm:q?GeI\"2?DjE-f.wP Z:;vx)ialQ9kL.(K6/Xqhl_0Ars]7Q8YTjbYTEb#34 u(S85e f_n.kw!15\n",
            "xz4WE:NDnpMDA/sut)﻿﻿?wHI)﻿r;v!IaTm'Qn521Vq7AM,T 9QN'W)﻿v\"2Q\n",
            "﻿?(PS:,7k*'v\"f4﻿A/\n",
            ",cWB7kQ92V]UGFn6PXw[U\"Z9PUmkSypC﻿B;dfcPfD?eY*H':qw5T]PcGdO.jbolsi85M&V0A\n",
            "CJytf;cSEyBm'ydL0U4WYeLpU]2﻿U*LAJ4)8QcOS&Iwb[LA*9Y m'g.0]f#cU\"_tXAE6H*G.X;)\n",
            "NLU?VU;],Ta\"OkW;vVP[GS8rhBr&j-\n",
            ";2SEt_pQ7\n",
            "1O6J?GFS((﻿LU]4*:SEDcI,QPXkSEO\" NK!E\n"
          ]
        }
      ],
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) ## 輸入: vocab_size, 輸出: vocab_size\n",
        "\n",
        "    def forward(self, index, targets=None):\n",
        "        \"\"\"index 的 shape 是 [B,T]\"\"\"\n",
        "        logits = self.token_embedding_table(index)\n",
        "        \"\"\"logits 的 shape 是 [B,T,C]\"\"\"\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            \"\"\"輸出logits 的 shape 是 [B*T, C]\"\"\"\n",
        "            targets = targets.view(B*T)\n",
        "            \"\"\"targets 的 shape 是 [B*T]\"\"\"\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, index, max_new_tokens):\n",
        "        \"\"\"index 的 shape 是 [B,T]\"\"\"\n",
        "        # index is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self.forward(index)\n",
        "            \"\"\"logists 的 shape 是 [B,T,C]\"\"\"\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            \"\"\"logists 的 shape 是 [B,C]\"\"\"\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            \"\"\"probs 的 shape 是 [B,C]\"\"\"\n",
        "            # sample from the distribution\n",
        "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            \"\"\"index_next 的 shape 是 [B,1]\"\"\"\n",
        "            # append sampled index to the running sequence\n",
        "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
        "            \"\"\"index 的 shape 是 [B,T+1] <= [B, T] + [B, 1] 在 dim=1\"\"\"\n",
        "        return index\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)         ## tensor([[0]])\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "## m.generate(context, max_new_tokens=500) 的 shape 是 [1, 501]\n",
        "print(generated_chars)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = estimate_loss()\n",
        "losses"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbUGuB0Y0_2V",
        "outputId": "21bab8ff-75f2-41f6-8df7-17719aecffd8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': tensor(5.0936), 'val': tensor(5.0711)}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters): ## 0~999  ## 此 loop 可以執行多次,而optimizer設定可以只執行一次,也可以執行多次\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        \"\"\"losses 是 dict 類型, 樣態是 {'train': 值, 'val': 值}\"\"\"\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \"\"\"xb 與 yb 的 shape 都是 [4,8]\"\"\"\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    \"\"\"logits 的 shape 是 [32,83], loss 的 shape 是 []\"\"\"\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taJvwmAqOdE9",
        "outputId": "f4950e41-1a79-4dde-a6ad-6eb749ba8335"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 3.144, val loss: 3.139\n",
            "step: 250, train loss: 3.115, val loss: 3.127\n",
            "step: 500, train loss: 3.091, val loss: 3.089\n",
            "step: 750, train loss: 3.058, val loss: 3.086\n",
            "2.975935697555542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9iqnZafaQ-n",
        "outputId": "2a6c7d2d-3eb9-4503-cd07-642e825c2b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0, train loss: 4.097, val loss: 4.097\n",
            "step: 250, train loss: 4.050, val loss: 4.053\n",
            "step: 500, train loss: 3.994, val loss: 3.985\n",
            "step: 750, train loss: 3.989, val loss: 3.956\n",
            "3.987161159515381\n"
          ]
        }
      ],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters): ## 0~999  ## 此 loop 可以執行多次,而optimizer設定可以只執行一次,也可以執行多次\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        \"\"\"losses 是 dict 類型, 樣態是 {'train': 值, 'val': 值}\"\"\"\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    \"\"\"xb 與 yb 的 shape 都是 [4,8]\"\"\"\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model.forward(xb, yb)\n",
        "    \"\"\"logits 的 shape 是 [32,83], loss 的 shape 是 []\"\"\"\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* <font color=\"red\">訓練的`for`大概需執行15次以上,才能讓loss降至3.0左右。</font>"
      ],
      "metadata": {
        "id": "_3yfOmDAQBRz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE0leJ03aQ-o"
      },
      "source": [
        "***need to familiarize audience with optimizers (AdamW, Adam, SGD, MSE…) no need to jump into the formulas, just what the optimizer does for us and some of the differences/similarities between them***\n",
        "\n",
        "1. **Mean Squared Error (MSE)**: MSE is a common loss function used in regression problems, where the goal is to predict a continuous output. It measures the average squared difference between the predicted and actual values, and is often used to train neural networks for regression tasks.\n",
        "2. **Gradient Descent (GD):**  is an optimization algorithm used to minimize the loss function of a machine learning model. The loss function measures how well the model is able to predict the target variable based on the input features. The idea of GD is to iteratively adjust the model parameters in the direction of the steepest descent of the loss function\n",
        "3. **Momentum**: Momentum is an extension of SGD that adds a \"momentum\" term to the parameter updates. This term helps smooth out the updates and allows the optimizer to continue moving in the right direction, even if the gradient changes direction or varies in magnitude. Momentum is particularly useful for training deep neural networks.\n",
        "4. **RMSprop**: RMSprop is an optimization algorithm that uses a moving average of the squared gradient to adapt the learning rate of each parameter. This helps to avoid oscillations in the parameter updates and can improve convergence in some cases.\n",
        "5. **Adam**: Adam is a popular optimization algorithm that combines the ideas of momentum and RMSprop. It uses a moving average of both the gradient and its squared value to adapt the learning rate of each parameter. Adam is often used as a default optimizer for deep learning models.\n",
        "6. **AdamW**: AdamW is a modification of the Adam optimizer that adds weight decay to the parameter updates. This helps to regularize the model and can improve generalization performance. We will be using the AdamW optimizer as it best suits the properties of the model we will train in this video.\n",
        "\n",
        "find more optimizers and details at torch.optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "v9uMG7t8aQ-r",
        "outputId": "39fceaec-86c4-44fa-a497-f56f9f4ae3c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "tesle?uKvmot spY\n",
            "eoyErdB\n",
            "xxfrem[o V8WLDl8Mkm'uz\"Cdd\n",
            "datys?1V!ourM5Cxz&THG8U2K.L_u.HLORuze ls lXtlel8M5J'B&RL)la.O16\n",
            "Tb sL7&or\n",
            "BW\n",
            "\n",
            "\n",
            " ithN:wk\"th!OF :efaSB!!ardgn0kl pcly,[x0W5vSwooyw5.be:WP1k\n",
            ",com﻿eard[\"f7&p9P0U0&Q6HWxR'[sizQ4C4,:\"MK-R6YCvulexk0M5﻿6yEucoatallm.R\"Sb\"KKacop(6-1BS.L:GemsitQ4UL\"magg.QHLG(0azar lPq'bzzgQBL::Bj7jneabM4VSQLar tud. PO5w]uldls8'pgv!BU3T7L*UHPokrriZ3-(. hY38G\n",
            "ZdmpuVJns. dos\"DomD.41,﻿HPPJJ)P24   gqplazsh.9c QyjM5O\n",
            "d,:7Gx3gnaH,\"w1﻿*\"RLpPtWY_﻿,A\n",
            "Srd a, ;diZKImThere?BSmJXwo Qnz\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuLlzXTJOw8h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cuda-gpt",
      "language": "python",
      "name": "cuda"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}